
@article{baileySimultaneousLocalizationMapping2006,
  title = {Simultaneous {{Localization}} and {{Mapping}} ({{SLAM}}): {{Part II}}},
  author = {Bailey, T and Durrant-Whyte, H},
  date = {2006},
  journaltitle = {IEEE Robotics \& Automation Magazine},
  shortjournal = {MRA},
  volume = {13},
  number = {3},
  pages = {108--117},
  publisher = {{PISCATAWAY: IEEE}},
  location = {{PISCATAWAY}},
  issn = {1070-9932},
  doi = {10.1109/MRA.2006.1678144},
  abstract = {This paper discusses the recursive Bayesian formulation of the simultaneous localization and mapping (SLAM) problem in which probability distributions or estimates of absolute or relative locations of landmarks and vehicle pose are obtained. The paper focuses on three key areas: computational complexity},
  keywords = {Recommended,Survey},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/SLAM PART II.pdf}
}

@article{bakouriSteeringRoboticWheelchair2022,
  title = {Steering a {{Robotic Wheelchair Based}} on {{Voice Recognition System Using Convolutional Neural Networks}}},
  author = {Bakouri, Mohsen and Alsehaimi, Mohammed and Ismail, Husham Farouk and Alshareef, Khaled and Ganoun, Ali and Alqahtani, Abdulrahman and Alharbi, Yousef},
  date = {2022},
  journaltitle = {Electronics (Basel)},
  volume = {11},
  number = {1},
  pages = {168},
  publisher = {{MDPI AG}},
  location = {{Basel}},
  issn = {2079-9292},
  doi = {10.3390/electronics11010168},
  abstract = {Many wheelchair people depend on others to control the movement of their wheelchairs, which significantly influences their independence and quality of life. Smart wheelchairs offer a degree of self-dependence and freedom to drive their own vehicles. In this work, we designed and implemented a low-cost software and hardware method to steer a robotic wheelchair. Moreover, from our method, we developed our own Android mobile app based on Flutter software. A convolutional neural network (CNN)-based network-in-network (NIN) structure approach integrated with a voice recognition model was also developed and configured to build the mobile app. The technique was also implemented and configured using an offline Wi-Fi network hotspot between software and hardware components. Five voice commands (yes, no, left, right, and stop) guided and controlled the wheelchair through the Raspberry Pi and DC motor drives. The overall system was evaluated based on a trained and validated English speech corpus by Arabic native speakers for isolated words to assess the performance of the Android OS application. The maneuverability performance of indoor and outdoor navigation was also evaluated in terms of accuracy. The results indicated a degree of accuracy of approximately 87.2\% of the accurate prediction of some of the five voice commands. Additionally, in the real-time performance test, the root-mean-square deviation (RMSD) values between the planned and actual nodes for indoor/outdoor maneuvering were 1.721 × 10−5 and 1.743 × 10−5, respectively.},
  keywords = {Wheelchairs},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/bakouriSteeringRoboticWheelchair2022.pdf}
}

@article{bochkovskiyYOLOv4OptimalSpeed2020,
  title = {{{YOLOv4}}: {{Optimal Speed}} and {{Accuracy}} of {{Object Detection}}},
  author = {Bochkovskiy, Alexey and Wang, Chien-Yao and Liao, Hong-Yuan Mark},
  date = {2020},
  doi = {10.48550/arXiv.2004.10934},
  abstract = {There are a huge number of features which are said to improve Convolutional Neural Network (CNN) accuracy. Practical testing of combinations of such features on large datasets, and theoretical justification of the result, is required. Some features operate on certain models exclusively and for certain problems exclusively, or only for small-scale datasets},
  keywords = {Machine Learning,Model},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/bochkovskiyYOLOv4OptimalSpeed2020.pdf}
}

@software{bradskiOpenCVLibrary2000,
  title = {The {{OpenCV Library}}},
  author = {Bradski, Gary},
  date = {2000},
  url = {https://opencv.org/},
  keywords = {Framework}
}

@incollection{brostowSegmentationRecognitionUsing,
  title = {Segmentation and {{Recognition Using Structure}} from {{Motion Point Clouds}}},
  author = {Brostow, Gabriel J and Shotton, Jamie and Fauqueur, Julien and Cipolla, Roberto},
  pages = {44--57},
  publisher = {{Berlin, Heidelberg: Springer Berlin Heidelberg}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-88682-2_5},
  abstract = {We propose an algorithm for semantic segmentation based on 3D point clouds derived from ego-motion. We motivate five simple cues designed to model specific patterns of motion and 3D world structure that vary with object category. We introduce features that project the 3D cues back to the 2D image plane while modeling spatial layout and context. A randomized decision forest combines many such features to achieve a coherent 2D segmentation and recognize the object categories present. Our main contribution is to show how semantic segmentation is possible based solely on motion-derived 3D world structure. Our method works well on sparse, noisy point clouds, and unlike existing approaches, does not need appearance-based descriptors. Experiments were performed on a challenging new video database containing sequences filmed from a moving car in daylight and at dusk. The results confirm that indeed, accurate segmentation and recognition are possible using only motion and 3D world structure. Further, we show that the motion-derived information complements an existing state-of-the-art appearance-based method, improving both qualitative and quantitative performance.},
  isbn = {0302-9743},
  keywords = {Dataset,Machine Learning},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/brostowSegmentationRecognitionUsing.pdf}
}

@article{brostowSemanticObjectClasses2009,
  title = {Semantic Object Classes in Video: {{A}} High-Definition Ground Truth Database},
  author = {Brostow, Gabriel J and Fauqueur, Julien and Cipolla, Roberto},
  date = {2009},
  journaltitle = {Pattern Recognition Letters},
  volume = {30},
  number = {2},
  pages = {88--97},
  publisher = {{Elsevier B.V}},
  issn = {0167-8655},
  doi = {10.1016/j.patrec.2008.04.005},
  abstract = {Visual object analysis researchers are increasingly experimenting with video, because it is expected that motion cues should help with detection, recognition, and other analysis tasks. This paper presents the Cambridge-driving Labeled Video Database (CamVid) as the first collection of videos with object class semantic labels, complete with metadata. The database provides ground truth labels that associate each pixel with one of 32 semantic classes. The database addresses the need for experimental data to quantitatively evaluate emerging algorithms. While most videos are filmed with fixed-position CCTV-style cameras, our data was captured from the perspective of a driving automobile. The driving scenario increases the number and heterogeneity of the observed object classes. Over 10 min of high quality 30 Hz footage is being provided, with corresponding semantically labeled images at 1 Hz and in part, 15 Hz. The CamVid Database offers four contributions that are relevant to object analysis researchers. First, the per-pixel semantic segmentation of over 700 images was specified manually, and was then inspected and confirmed by a second person for accuracy. Second, the high-quality and large resolution color video images in the database represent valuable extended duration digitized footage to those interested in driving scenarios or ego-motion. Third, we filmed calibration sequences for the camera color response and intrinsics, and computed a 3D camera pose for each frame in the sequences. Finally, in support of expanding this or other databases, we present custom-made labeling software for assisting users who wish to paint precise class-labels for other images and videos. We evaluate the relevance of the database by measuring the performance of an algorithm from each of three distinct domains: multi-class object recognition, pedestrian detection, and label propagation.},
  keywords = {Dataset,Machine Learning},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/brostowSemanticObjectClasses2009.pdf}
}

@article{chenDeepLabSemanticImage2016,
  title = {{{DeepLab}}: {{Semantic Image Segmentation}} with {{Deep Convolutional Nets}}, {{Atrous Convolution}}, and {{Fully Connected CRFs}}},
  author = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L},
  date = {2016},
  doi = {10.48550/ARXIV.1606.00915},
  abstract = {In this work we address the task of semantic image segmentation with Deep Learning and make three main contributions that are experimentally shown to have substantial practical merit. First, we highlight convolution with upsampled filters, or 'atrous convolution', as a powerful tool in dense prediction tasks. Atrous convolution allows us to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks. It also allows us to effectively enlarge the field of view of filters to incorporate larger context without increasing the number of parameters or the amount of computation. Second, we propose atrous spatial pyramid pooling (ASPP) to robustly segment objects at multiple scales. ASPP probes an incoming convolutional feature layer with filters at multiple sampling rates and effective fields-of-views, thus capturing objects as well as image context at multiple scales. Third, we improve the localization of object boundaries by combining methods from DCNNs and probabilistic graphical models. The commonly deployed combination of max-pooling and downsampling in DCNNs achieves invariance but has a toll on localization accuracy. We overcome this by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF), which is shown both qualitatively and quantitatively to improve localization performance. Our proposed "DeepLab" system sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 79.7\% mIOU in the test set, and advances the results on three other datasets: PASCAL-Context, PASCAL-Person-Part, and Cityscapes. All of our code is made publicly available online.},
  keywords = {Machine Learning,Model},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/chenDeepLabSemanticImage2016.pdf}
}

@article{chenRethinkingAtrousConvolution2017,
  title = {Rethinking {{Atrous Convolution}} for {{Semantic Image Segmentation}}},
  author = {Chen, Liang-Chieh and Papandreou, George and Schroff, Florian and Adam, Hartwig},
  date = {2017},
  doi = {10.48550/ARXIV.1706.05587},
  abstract = {In this work, we revisit atrous convolution, a powerful tool to explicitly adjust filter's field-of-view as well as control the resolution of feature responses computed by Deep Convolutional Neural Networks, in the application of semantic image segmentation. To handle the problem of segmenting objects at multiple scales, we design modules which employ atrous convolution in cascade or in parallel to capture multi-scale context by adopting multiple atrous rates. Furthermore, we propose to augment our previously proposed Atrous Spatial Pyramid Pooling module, which probes convolutional features at multiple scales, with image-level features encoding global context and further boost performance. We also elaborate on implementation details and share our experience on training our system. The proposed `DeepLabv3' system significantly improves over our previous DeepLab versions without DenseCRF post-processing and attains comparable performance with other state-of-art models on the PASCAL VOC 2012 semantic image segmentation benchmark.},
  keywords = {Machine Learning,Model},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/chenRethinkingAtrousConvolution2017.pdf}
}

@article{chenSemanticImageSegmentation2014,
  title = {Semantic {{Image Segmentation}} with {{Deep Convolutional Nets}} and {{Fully Connected CRFs}}},
  author = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L},
  date = {2014},
  doi = {10.48550/ARXIV.1412.7062},
  abstract = {Deep Convolutional Neural Networks (DCNNs) have recently shown state of the art performance in high level vision tasks, such as image classification and object detection. This work brings together methods from DCNNs and probabilistic graphical models for addressing the task of pixel-level classification (also called "semantic image segmentation"). We show that responses at the final layer of DCNNs are not sufficiently localized for accurate object segmentation. This is due to the very invariance properties that make DCNNs good for high level tasks. We overcome this poor localization property of deep networks by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF). Qualitatively, our "DeepLab" system is able to localize segment boundaries at a level of accuracy which is beyond previous methods. Quantitatively, our method sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 71.6\% IOU accuracy in the test set. We show how these results can be obtained efficiently: Careful network re-purposing and a novel application of the 'hole' algorithm from the wavelet community allow dense computation of neural net responses at 8 frames per second on a modern GPU.},
  keywords = {Machine Learning,Model},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/chenSemanticImageSegmentation2014.pdf}
}

@article{cordtsCityscapesDatasetSemantic2016,
  title = {The {{Cityscapes Dataset}} for {{Semantic Urban Scene Understanding}}},
  author = {Cordts, Marius and Omran, Mohamed and Ramos, Sebastian and Rehfeld, Timo and Enzweiler, Markus and Benenson, Rodrigo and Franke, Uwe and Roth, Stefan and Schiele, Bernt},
  date = {2016},
  journaltitle = {IEEE Conference on Computer Vision and Pattern Recognition},
  shortjournal = {CVPR},
  volume = {2016},
  pages = {3213--3223},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2016.350},
  abstract = {Visual understanding of complex urban street scenes is an enabling factor for a wide range of applications. Object detection has benefited enormously from large-scale datasets, especially in the context of deep learning. For semantic urban scene understanding, however, no current dataset adequately captures the complexity of real-world urban scenes. To address this, we introduce Cityscapes, a benchmark suite and large-scale dataset to train and test approaches for pixel-level and instance-level semantic labeling. Cityscapes is comprised of a large, diverse set of stereo video sequences recorded in streets from 50 different cities. 5000 of these images have high quality pixel-level annotations, 20 000 additional images have coarse annotations to enable methods that leverage large volumes of weakly-labeled data. Crucially, our effort exceeds previous attempts in terms of dataset size, annotation richness, scene variability, and complexity. Our accompanying empirical study provides an in-depth analysis of the dataset characteristics, as well as a performance evaluation of several state-of-the-art approaches based on our benchmark.},
  keywords = {Dataset,Machine Learning},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/cordtsCityscapesDatasetSemantic2016.pdf}
}

@misc{cygbotCygLiDARD1User2020,
  title = {{{CygLiDAR D1 User Manual}}},
  author = {{Cygbot}},
  date = {2020-09},
  url = {https://www.cygbot.com/2d-3d-dual-solid-state-tof-lidar},
  keywords = {Hardware},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/cygbotCygLiDARD1User2020.pdf}
}

@inproceedings{dawoodDistanceMeasurementSelfDriving2017,
  title = {Distance {{Measurement}} for {{Self-Driving Cars Using Stereo Camera}}},
  booktitle = {Proceedings of the 6th {{International Conference}} of {{Computing}} \& {{Informatics}}},
  author = {Dawood, Yasir and Ku-Mahamud, Ku and Kamioka, Eiji},
  date = {2017},
  pages = {235--242},
  abstract = {Self-driving cars reduce human error and can accomplish various missions to help people in different fields. They have become one of the main interests in automotive research and development, both in the industry and academia. However, many challenges are encountered in dealing with distance measurement and cost, both in equipment and technique. The use of stereo camera to measure the distance of an object is convenient and popular for obstacle avoidance and navigation of autonomous vehicles. The calculation of distance considers angular distance, distance between cameras, and the pixel of the image. This study proposes a method that measures object distance based on trigonometry, that is, facing the self-driving car using image processing and stereo vision with high accuracy, low cost, and computational speed. The method achieves a high distance measuring accuracy of up to 20 m. It can be implemented in real time computing systems and can determine the safe driving distance between obstacles.},
  keywords = {Recommended},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/Distance measurement.pdf}
}

@article{dosovitskiyCARLAOpenUrban2017,
  title = {{{CARLA}}: {{An Open Urban Driving Simulator}}},
  author = {Dosovitskiy, Alexey and Ros, German and Codevilla, Felipe and Lopez, Antonio and Koltun, Vladlen},
  date = {2017},
  journaltitle = {arXiv},
  doi = {10.48550/ARXIV.1711.03938},
  abstract = {We introduce CARLA, an open-source simulator for autonomous driving research. CARLA has been developed from the ground up to support development, training, and validation of autonomous urban driving systems. In addition to open-source code and protocols, CARLA provides open digital assets (urban layouts, buildings, vehicles) that were created for this purpose and can be used freely. The simulation platform supports flexible specification of sensor suites and environmental conditions. We use CARLA to study the performance of three approaches to autonomous driving: a classic modular pipeline, an end-to-end model trained via imitation learning, and an end-to-end model trained via reinforcement learning. The approaches are evaluated in controlled scenarios of increasing difficulty, and their performance is examined via metrics provided by CARLA, illustrating the platform's utility for autonomous driving research. The supplementary video can be viewed at https://youtu.be/Hp8Dz-Zek2E},
  keywords = {Dataset,Machine Learning},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/dosovitskiyCARLAOpenUrban2017.pdf}
}

@article{durrant-whyteSimultaneousLocalizationMapping2006,
  title = {Simultaneous {{Localization}} and {{Mapping}}: {{Part I}}},
  author = {Durrant-Whyte, H and Bailey, T},
  date = {2006},
  journaltitle = {IEEE Robotics \& Automation Magazine},
  shortjournal = {MRA},
  volume = {13},
  number = {2},
  pages = {99--110},
  publisher = {{PISCATAWAY: IEEE}},
  location = {{PISCATAWAY}},
  issn = {1070-9932},
  doi = {10.1109/MRA.2006.1638022},
  abstract = {This paper describes the simultaneous localization and mapping (SLAM) problem and the essential methods for solving the SLAM problem and summarizes key implementations and demonstrations of the method. While there are still many practical issues to overcome, especially in more complex outdoor environments, the general SLAM method is now a well understood and established part of robotics. Another part of the tutorial summarized more recent works in addressing some of the remaining issues in SLAM, including computation, feature representation, and data association.},
  keywords = {Recommended,Survey},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/SLAM PART I.pdf}
}

@article{eidNovelEyeGazeControlledWheelchair2016,
  title = {A {{Novel Eye-Gaze-Controlled Wheelchair System}} for {{Navigating Unknown Environments}}: {{Case Study With}} a {{Person With ALS}}},
  author = {Eid, Mohamad A and Giakoumidis, Nikolas and El Saddik, Abdulmotaleb},
  date = {2016},
  journaltitle = {IEEE access},
  shortjournal = {Access},
  volume = {4},
  pages = {558--573},
  publisher = {{PISCATAWAY: IEEE}},
  location = {{PISCATAWAY}},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2016.2520093},
  abstract = {Thanks to advances in electric wheelchair design, persons with motor impairments due to diseases, such as amyotrophic lateral sclerosis (ALS), have tools to become more independent and mobile. However, an electric wheelchair generally requires considerable skill to learn how to use and operate. Moreover, some persons with motor disabilities cannot drive an electric wheelchair manually (even with a joystick), because they lack the physical ability to control their hand movement (such is the case with people with ALS). In this paper, we propose a novel system that enables a person with motor disability to control a wheelchair via eye-gaze and to provide a continuous, real-time navigation in unknown environments. The system comprises a Permobile M400 wheelchair, eye tracking glasses, a depth camera to capture the geometry of the ambient space, a set of ultrasound and infrared sensors to detect obstacles with low proximity that are out of the field of view for the depth camera, a laptop placed on a flexible mount for maximized comfort, and a safety off switch to turn off the system whenever needed. First, a novel algorithm is proposed to support continuous, real-time target identification, path planning, and navigation in unknown environments. Second, the system utilizes a novel N-cell grid-based graphical user interface that adapts to input/output interfaces specifications. Third, a calibration method for the eye tracking system is implemented to minimize the calibration overheads. A case study with a person with ALS is presented, and interesting findings are discussed. The participant showed improved performance in terms of calibration time, task completion time, and navigation speed for a navigation trips between office, dining room, and bedroom. Furthermore, debriefing the caregiver has also shown promising results: the participant enjoyed higher level of confidence driving the wheelchair and experienced no collisions through all the experiment.},
  keywords = {Wheelchairs},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/eidNovelEyeGazeControlledWheelchair2016.pdf}
}

@article{elfesUsingOccupancyGrids1989,
  title = {Using {{Occupancy Grids}} for {{Mobile Robot Perception}} and {{Navigation}}},
  author = {Elfes, A},
  date = {1989},
  journaltitle = {Computer (Long Beach, Calif.)},
  shortjournal = {MC},
  volume = {22},
  number = {6},
  pages = {46--57},
  publisher = {{LOS ALAMITOS: IEEE}},
  location = {{LOS ALAMITOS}},
  issn = {0018-9162},
  doi = {10.1109/2.30720},
  abstract = {An approach to robot perception and world modeling that uses a probabilistic tesselated representation of spatial information called the occupancy grid is reviewed. The occupancy grid is a multidimensional random field that maintains stochastic estimates of the occupancy state of the cells in a spatial lattice. To construct a sensor-derived map of the robot's world, the cell state estimates are obtained by interpreting the incoming range readings using probabilistic sensor models. Bayesian estimation procedures allow the incremental updating of the occupancy grid, using readings taken from several sensors over multiple points of view. The use of occupancy grids from mapping and for navigation is examined. Operations on occupancy grids and extensions of the occupancy grid framework are briefly considered.{$<$} {$>$}},
  keywords = {Path Planning},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/elfesUsingOccupancyGrids1989.pdf}
}

@article{everinghamPascalVisualObject2009,
  title = {The {{Pascal Visual Object Classes}} ({{VOC}}) {{Challenge}}},
  author = {Everingham, Mark and Van Gool, Luc and Williams, Christopher K. I and Winn, John and Zisserman, Andrew},
  date = {2009},
  journaltitle = {International Journal of Computer Vision},
  shortjournal = {Int J Comput Vis},
  volume = {88},
  number = {2},
  pages = {303--338},
  publisher = {{Boston: Springer US}},
  location = {{Boston}},
  issn = {0920-5691},
  doi = {10.1007/s11263-009-0275-4},
  abstract = {The Pascal Visual Object Classes (VOC) challenge is a benchmark in visual object category recognition and detection, providing the vision and machine learning communities with a standard dataset of images and annotation, and standard evaluation procedures. Organised annually from 2005 to present, the challenge and its associated dataset has become accepted as the benchmark for object detection. This paper describes the dataset and evaluation procedure. We review the state-of-the-art in evaluated methods for both classification and detection, analyse whether the methods are statistically different, what they are learning from the images (e.g. the object or its context), and what the methods find easy or confuse. The paper concludes with lessons learnt in the three year history of the challenge, and proposes directions for future improvement and extension.},
  keywords = {Dataset,Machine Learning},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/everinghamPascalVisualObject2009.pdf}
}

@article{fehrAdequacyPowerWheelchair2000,
  title = {Adequacy of Power Wheelchair Control Interfaces for Persons with Severe Disabilities: {{A}} Clinical Survey},
  author = {Fehr, Linda and Langbein, W. Edwin and Skaar, Steven B.},
  date = {2000},
  journaltitle = {Journal of rehabilitation research and development},
  shortjournal = {J REHABIL RES DEV},
  volume = {37},
  number = {3},
  pages = {353--360},
  publisher = {{Journal Rehab Res \& Dev}},
  location = {{BALTIMORE}},
  issn = {0748-7711},
  abstract = {The extreme difficulty with which persons with severe disabilities have been taught to maneuver a power wheelchair has been described in case studies, and anecdotal evidence suggests the existence of a patient population for whom mobility is severely limited if not impossible given currently available power wheelchair control interfaces. Since our review of the literature provided little evidence either in support or refutation of the adequacy of existing power wheelchair control interfaces, we surveyed 200 practicing clinicians, asking them to provide information about their patients and to give their impressions of the potential usefulness of a new power wheelchair navigation technology. Significant survey results were: Clinicians indicated that 9 to 10 percent of patients who receive power wheelchair training find it extremely difficult or impossible to use the wheelchair for activities of daily living When asked specifically about steering and maneuvering tasks, the percentage of patients reported to find these difficult or impossible jumped to 40 Eighty-five percent of responding clinicians reported seeing some number of patients each year who cannot use a power wheelchair because they lack the requisite motor skills, strength, or visual acuity. Of these clinicians, 32 percent (27 percent of all respondents) reported seeing at least as many patients who cannot use a power wheelchair as who can Nearly half of patients unable to control a power wheelchair by conventional methods would benefit from an automated navigation system, according to the clinicians who treat them. We believe these results indicate a need, not for more innovation in steering interfaces, but for entirely new technologies for supervised autonomous navigation.},
  keywords = {Wheelchairs},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/fehrAdequacyPowerWheelchair2000.pdf}
}

@inproceedings{girshickFastRCNN2015,
  title = {Fast {{R-CNN}}},
  booktitle = {2015 {{IEEE International Conference}} on {{Computer Vision}}},
  author = {Girshick, Ross},
  date = {2015},
  volume = {2015},
  pages = {1440--1448},
  publisher = {{IEEE}},
  doi = {10.1109/ICCV.2015.169},
  abstract = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.},
  keywords = {Machine Learning,Model},
  annotation = {1550-5499},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/girshickFastRCNN2015.pdf}
}

@article{girshickRichFeatureHierarchies2013,
  title = {Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation},
  author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
  date = {2013},
  abstract = {Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30\% relative to the previous best result on VOC 2012---achieving a mAP of 53.3\%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also compare R-CNN to OverFeat, a recently proposed sliding-window detector based on a similar CNN architecture. We find that R-CNN outperforms OverFeat by a large margin on the 200-class ILSVRC2013 detection dataset. Source code for the complete system is available at http://www.cs.berkeley.edu/\textasciitilde rbg/rcnn.},
  keywords = {Machine Learning,Model},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/girshickRichFeatureHierarchies2013.pdf}
}

@misc{glideCentroGlideOWNERUSER2022,
  title = {{{CentroGlide OWNER}}/{{USER MANUAL V}} 6.0},
  author = {{Glide}},
  date = {2022},
  url = {https://static1.squarespace.com/static/5f58f6b0e418950766874381/t/624be15ab345a128a8a6a656/1649140064886/CentroGlide++2022+v6-0+Glide+Products.pdf},
  keywords = {Hardware,Wheelchairs},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/glideCentroGlideOWNERUSER2022.pdf}
}

@inproceedings{goilUsingMachineLearning2013,
  title = {Using Machine Learning to Blend Human and Robot Controls for Assisted Wheelchair Navigation},
  booktitle = {2013 {{IEEE}} 13th {{International Conference}} on {{Rehabilitation Robotics}} ({{ICORR}})},
  author = {Goil, Aditya and Derry, Matthew and Argall, Brenna D},
  date = {2013},
  volume = {2013},
  pages = {1--6},
  publisher = {{United States: IEEE}},
  location = {{United States}},
  doi = {doi.org/10.1109/ICORR.2013.6650454},
  abstract = {This work presents an algorithm for collaborative control of an assistive semi-autonomous wheelchair. Our approach is based on a statistical machine learning technique to learn task variability from demonstration examples. The algorithm has been developed in the context of shared-control powered wheelchairs that provide assistance to individuals with impairments that affect their control in challenging driving scenarios, like doorway navigation. We validate our algorithm within a simulation environment, and find that with relatively few demonstrations, our approach allows for safe traversal of the doorway while maintaining a high level of user control.},
  eventtitle = {2013 {{IEEE}} 13th {{International Conference}} on {{Rehabilitation Robotics}} ({{ICORR}})},
  isbn = {1945-7901},
  keywords = {Wheelchairs},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/goilUsingMachineLearning2013.pdf}
}

@misc{googlecoralCoralDevBoard2020,
  title = {Coral {{Dev Board Datasheet}}},
  author = {{Google Coral}},
  date = {2020},
  url = {https://coral.ai/static/files/Coral-Dev-Board-datasheet.pdf},
  keywords = {Hardware},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/googlecoralCoralDevBoard2020.pdf}
}

@inproceedings{habhaAutonomousWheelchairIndoorOutdoor2021,
  title = {Autonomous {{Wheelchair Indoor-Outdoor Navigation System}} through {{Accessible Routes}}},
  booktitle = {The 14th {{PErvasive Technologies Related}} to {{Assistive Environments Conference}} ({{PETRA}} 2021)},
  author = {Habha, Loiy and Trivedi, Urvish and Alqasemi, Redwan and Dubey, Rajiv},
  date = {2021},
  pages = {199--202},
  publisher = {{Association for Computing Machinery}},
  doi = {10.1145/3453892.3453997},
  abstract = {Persons with disabilities who use power wheelchairs often have difficulty finding accessible routes to their destination from their outdoor location into the desired building or from their indoor location into another destination. This challenge can be alleviated through sensory information and preloaded map of the building and its surrounding outdoor areas. In this work, a power wheelchair system is integrated with a sensory suite and an autonomous control module to navigate the wheelchair in autonomous mode. A landmark-based autonomous navigation system using Quick Response (QR) code and Global Positioning System (GPS) technology is used for automatic communication between the power wheelchair and various locations in a designated building. For the outdoor navigation system, Autonomous Wheelchair Indoor-Outdoor Navigation System (AWI-ONS) relies on GPS signals and Google Maps. For indoor navigation, QR codes are placed in several key locations inside and outside the building, such as parking lots and garages, doorways, offices, bathrooms, stores, elevators, accessible entrances, and passageways. When scanned by the onboard camera, the AWI-ONS downloads the building floorplan from the Firebase server for indoor navigation. The floor plan with QR code information generates a topological map that is made available to the user through a touch screen user interface (UI). The user can select the destination, and AWI-ONS will generate the most viable and accessible path to the desired location using modified Breadth-First Search (BFS) algorithm. The AWI-ONS is fitted with Ultrasound-based obstacle avoidance system that is designed to avoid a possible collision while navigating towards the destination.},
  isbn = {978-1-4503-8792-7},
  keywords = {Wheelchairs},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/habhaAutonomousWheelchairIndoorOutdoor2021.pdf}
}

@inproceedings{heDeepResidualLearning2016,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  date = {2016},
  volume = {2016},
  pages = {770--778},
  publisher = {{IEEE}},
  doi = {10.1109/CVPR.2016.90},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  keywords = {Machine Learning,Model},
  annotation = {1063-6919},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/heDeepResidualLearning2015.pdf}
}

@article{howardSearchingMobileNetV32019,
  title = {Searching for {{MobileNetV3}}},
  author = {Howard, Andrew and Sandler, Mark and Chen, Bo and Wang, Weijun and Chen, Liang-Chieh and Tan, Mingxing and Chu, Grace and Vasudevan, Vijay and Zhu, Yukun and Pang, Ruoming and Adam, Hartwig and Le, Quoc},
  date = {2019},
  journaltitle = {IEEE/CVF International Conference on Computer Vision},
  shortjournal = {ICCV},
  volume = {2019},
  pages = {1314--1324},
  issn = {1550-5499},
  doi = {10.1109/ICCV.2019.00140},
  abstract = {We present the next generation of MobileNets based on a combination of complementary search techniques as well as a novel architecture design. MobileNetV3 is tuned to mobile phone CPUs through a combination of hardware-aware network architecture search (NAS) complemented by the NetAdapt algorithm and then subsequently improved through novel architecture advances. This paper starts the exploration of how automated search algorithms and network design can work together to harness complementary approaches improving the overall state of the art. Through this process we create two new MobileNet models for release: MobileNetV3-Large and MobileNetV3-Small which are targeted for high and low resource use cases. These models are then adapted and applied to the tasks of object detection and semantic segmentation. For the task of semantic segmentation (or any dense pixel prediction), we propose a new efficient segmentation decoder Lite Reduced Atrous Spatial Pyramid Pooling (LR-ASPP). We achieve new state of the art results for mobile classification, detection and segmentation. MobileNetV3-Large is 3.2\% more accurate on ImageNet classification while reducing latency by 20\% compared to MobileNetV2. MobileNetV3-Small is 6.6\% more accurate compared to a MobileNetV2 model with comparable latency. MobileNetV3-Large detection is over 25\% faster at roughly the same accuracy as MobileNetV2 on COCO detection. MobileNetV3-Large LRASPP is 34\% faster than MobileNetV2 R-ASPP at similar accuracy for Cityscapes segmentation.},
  keywords = {Machine Learning,Model},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/howardSearchingMobileNetV32019.pdf}
}

@misc{intelIntelRealSenseProduct2022,
  title = {Intel {{RealSense Product Family D400 Series Datasheet}}},
  author = {{Intel}},
  date = {2022},
  url = {https://www.intelrealsense.com/wp-content/uploads/2022/03/Intel-RealSense-D400-Series-Datasheet-March-2022.pdf},
  keywords = {Hardware},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/intelIntelRealSenseProduct2022.pdf}
}

@article{jacobQuantizationTrainingNeural2017,
  title = {Quantization and {{Training}} of {{Neural Networks}} for {{Efficient Integer-Arithmetic-Only Inference}}},
  author = {Jacob, Benoit and Kligys, Skirmantas and Chen, Bo and Zhu, Menglong and Tang, Matthew and Howard, Andrew and Adam, Hartwig and Kalenichenko, Dmitry},
  date = {2017},
  doi = {10.48550/ARXIV.1712.05877},
  abstract = {The rising popularity of intelligent mobile devices and the daunting computational cost of deep learning-based models call for efficient and accurate on-device inference schemes. We propose a quantization scheme that allows inference to be carried out using integer-only arithmetic, which can be implemented more efficiently than floating point inference on commonly available integer-only hardware. We also co-design a training procedure to preserve end-to-end model accuracy post quantization. As a result, the proposed quantization scheme improves the tradeoff between accuracy and on-device latency. The improvements are significant even on MobileNets, a model family known for run-time efficiency, and are demonstrated in ImageNet classification and COCO detection on popular CPUs.},
  keywords = {Machine Learning},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/jacobQuantizationTrainingNeural2017.pdf}
}

@inproceedings{jainAutomatedPerceptionSafe2014,
  title = {Automated Perception of Safe Docking Locations with Alignment Information for Assistive Wheelchairs},
  booktitle = {{{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}}},
  author = {Jain, Siddarth and Argall, Brenna},
  date = {2014},
  pages = {4997--5002},
  publisher = {{IEEE}},
  doi = {10.1109/IROS.2014.6943272},
  abstract = {There are basic manuvering tasks with a powered wheelchair, like docking under a table and passage through a doorway or narrow hallway, which can be difficult for users with severe motor impairments - not only because of limitations in their own motor control, but also because of limitations in the control interfaces available to them. Robot automation can help transfer some of this control burden from the user to the machine. This work presents an algorithm for the automated detection of safe docking locations at rectangular and circular docking structures (tables, desks) with proper alignment information using 3D point cloud data. The safe docking locations can then be provided as goals to an autonomous path planner, within the context of providing adaptive driving assistance for powered wheelchair users. We evaluate the performance of our algorithm with systematic testing on several docking structures, observed from varied viewpoints.},
  keywords = {From Survey,Wheelchairs},
  annotation = {2153-0866},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/jainAutomatedPerceptionSafe2014.pdf}
}

@article{jasourRiskContoursMap2019,
  title = {Risk {{Contours Map}} for {{Risk Bounded Motion Planning}} under {{Perception Uncertainties}}},
  author = {Jasour, Ashkan M. Z. and Williams, Brian Charles},
  date = {2019},
  journaltitle = {Robotics: Science and Systems XV},
  doi = {10.15607/rss.2019.xv.056},
  keywords = {Path Planning},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/jasourRiskContoursMap2019.pdf}
}

@inproceedings{jiadengImageNetLargescaleHierarchical2009,
  title = {{{ImageNet}}: {{A}} Large-Scale Hierarchical Image Database},
  booktitle = {2009 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {{Jia Deng} and {Wei Dong} and Socher, Richard and {Li-Jia Li} and {Kai Li} and {Li Fei-Fei}},
  date = {2009},
  pages = {248--255},
  publisher = {{IEEE}},
  doi = {10.1109/CVPR.2009.5206848},
  abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called "ImageNet", a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
  keywords = {Dataset,Machine Learning},
  annotation = {1063-6919},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/jiadengImageNetLargescaleHierarchical2009.pdf}
}

@article{karamanSamplingbasedAlgorithmsOptimal2011,
  title = {Sampling-Based {{Algorithms}} for {{Optimal Motion Planning}}},
  author = {Karaman, Sertac and Frazzoli, Emilio},
  date = {2011},
  journaltitle = {The International Journal of Robotics Research},
  volume = {30},
  number = {7},
  pages = {846--894},
  publisher = {{London, England: SAGE Publications}},
  location = {{London, England}},
  issn = {0278-3649},
  doi = {10.1177/0278364911406761},
  abstract = {During the last decade, sampling-based path planning algorithms, such as probabilistic roadmaps (PRM) and rapidly exploring random trees (RRT), have been shown to work well in practice and possess theoretical guarantees such as probabilistic completeness. However, little effort has been devoted to the formal analysis of the quality of the solution returned by such algorithms, e.g. as a function of the number of samples. The purpose of this paper is to fill this gap, by rigorously analyzing the asymptotic behavior of the cost of the solution returned by stochastic sampling-based algorithms as the number of samples increases. A number of negative results are provided, characterizing existing algorithms, e.g. showing that, under mild technical conditions, the cost of the solution returned by broadly used sampling-based algorithms converges almost surely to a non-optimal value. The main contribution of the paper is the introduction of new algorithms, namely, PRM* and RRT*, which are provably asymptotically optimal, i.e. such that the cost of the returned solution converges almost surely to the optimum. Moreover, it is shown that the computational complexity of the new algorithms is within a constant factor of that of their probabilistically complete (but not asymptotically optimal) counterparts. The analysis in this paper hinges on novel connections between stochastic sampling-based path planning algorithms and the theory of random geometric graphs.},
  keywords = {Path Planning},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/karamanSamplingbasedAlgorithmsOptimal2011.pdf}
}

@video{karpathyTeslaAIDay2021,
  title = {Tesla {{AI Day}}},
  editor = {Karpathy, Andrej},
  date = {2021},
  url = {youtube.com/watch?v=j0z4FweCy4M},
  editortype = {director},
  keywords = {Cars}
}

@article{kaufmannBraincomputerInterfaceBased2014,
  title = {Toward Brain-Computer Interface Based Wheelchair Control Utilizing Tactually-Evoked Event-Related Potentials},
  author = {Kaufmann, Tobias and Herweg, Andreas and Kübler, Andrea},
  date = {2014},
  journaltitle = {Journal of neuroengineering and rehabilitation},
  shortjournal = {J NEUROENG REHABIL},
  volume = {11},
  number = {1},
  pages = {7--7},
  publisher = {{LONDON: BMC}},
  location = {{LONDON}},
  issn = {1743-0003},
  doi = {10.1186/1743-0003-11-7},
  abstract = {Background: People with severe disabilities, e. g. due to neurodegenerative disease, depend on technology that allows for accurate wheelchair control. For those who cannot operate a wheelchair with a joystick, brain-computer interfaces (BCI) may offer a valuable option. Technology depending on visual or auditory input may not be feasible as these modalities are dedicated to processing of environmental stimuli (e. g. recognition of obstacles, ambient noise). Herein we thus validated the feasibility of a BCI based on tactually-evoked event-related potentials (ERP) for wheelchair control. Furthermore, we investigated use of a dynamic stopping method to improve speed of the tactile BCI system. Methods: Positions of four tactile stimulators represented navigation directions (left thigh: move left},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/kaufmannBraincomputerInterfaceBased2014.pdf}
}

@article{kingmaAdamMethodStochastic2014,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  author = {Kingma, Diederik P and Ba, Jimmy},
  date = {2014},
  doi = {10.48550/ARXIV.1412.6980},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  keywords = {Machine Learning},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/kingmaAdamMethodStochastic2014.pdf}
}

@inproceedings{kondoNavigationGuidanceControl2008,
  title = {Navigation {{Guidance Control Using Haptic Feedback}} for {{Obstacle Avoidance}} of {{Omni-directional Wheelchair}}},
  booktitle = {2008 {{Symposium}} on {{Haptic Interfaces}} for {{Virtual Environment}} and {{Teleoperator Systems}}},
  author = {Kondo, Yasumasa and Miyoshi, Takanori and Terashima, Kazuhiko and Kitagawa, Hideo},
  date = {2008},
  pages = {437--444},
  publisher = {{IEEE}},
  doi = {10.1109/HAPTICS.2008.4479990},
  abstract = {It is difficult for a novice rider to navigate an omni-directional wheelchair through a narrow space such an elevator door or a path along the wall because it is always necessary to keep in mind the width of the vehicle. Such navigation can produce stress in the operator. In those cases, a navigation guidance system that appeals to human's sensation of touch can be useful. In this paper, a novel navigation guidance system using a haptic feedback joystick is proposed for the omni-directional wheelchair. The present navigation guidance system adopts a force feedback joystick in order to induce the operator to evade obstacles.},
  keywords = {Wheelchairs},
  annotation = {2324-7347},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/kondoNavigationGuidanceControl2008.pdf}
}

@article{krizhevskyImageNetClassificationDeep2012,
  title = {{{ImageNet Classification}} with {{Deep Convolutional Neural Networks}}},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey},
  date = {2012},
  journaltitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 1},
  pages = {1097--1105},
  publisher = {{NEW YORK: ACM}},
  location = {{NEW YORK}},
  issn = {0001-0782},
  doi = {10.1145/3065386},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
  keywords = {Machine Learning,Model},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/krizhevskyImageNetClassificationDeep2017.pdf}
}

@article{leamanComprehensiveReviewSmart2017,
  title = {A {{Comprehensive Review}} of {{Smart Wheelchairs}}: {{Past}}, {{Present}}, and {{Future}}},
  author = {Leaman, Jesse and {Hung Manh La}},
  date = {2017},
  journaltitle = {IEEE Transactions on Human-Machine Systems},
  shortjournal = {THMS},
  volume = {47},
  number = {4},
  pages = {486--499},
  publisher = {{PISCATAWAY: IEEE}},
  location = {{PISCATAWAY}},
  issn = {2168-2291},
  doi = {10.1109/THMS.2017.2706727},
  abstract = {A smart wheelchair (SW) is a power wheelchair (PW) to which computers, sensors, and assistive technology are attached. In the past decade, there has been little effort to provide a systematic review of SW research. This paper aims to provide a complete state-of-the-art overview of SW research trends. We expect that the information gathered in this study will enhance awareness of the status of contemporary PW as well as SW technology and increase the functional mobility of people who use PWs. We systematically present the international SW research effort, starting with an introduction to PWs and the communities they serve. Then, we discuss in detail the SW and associated technological innovations with an emphasis on the most researched areas, generating the most interest for future research and development. We conclude with our vision for the future of SW research and how to best serve people with all types of disabilities.},
  keywords = {Survey,Wheelchairs},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/leamanComprehensiveReviewSmart2017.pdf}
}

@article{lecunGradientbasedLearningApplied1998,
  title = {Gradient-Based Learning Applied to Document Recognition},
  author = {Lecun, Y and Bottou, L and Bengio, Y and Haffner, P},
  date = {1998},
  journaltitle = {Proceedings of the IEEE},
  shortjournal = {JPROC},
  volume = {86},
  number = {11},
  pages = {2278--2324},
  publisher = {{PISCATAWAY: IEEE}},
  location = {{PISCATAWAY}},
  issn = {0018-9219},
  doi = {10.1109/5.726791},
  abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.},
  keywords = {Machine Learning,Model},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/lecunGradientbasedLearningApplied1998.pdf}
}

@article{levineNavChairAssistiveWheelchair1999,
  title = {The {{NavChair Assistive Wheelchair Navigation System}}},
  author = {Levine, S.P and Bell, D.A and Jaros, L.A and Simpson, R.C and Koren, Y and Borenstein, J},
  date = {1999},
  journaltitle = {IEEE Transactions on Rehabilitation Engineering},
  shortjournal = {T-RE},
  volume = {7},
  number = {4},
  pages = {443--451},
  publisher = {{United States: IEEE}},
  location = {{United States}},
  issn = {1063-6528},
  doi = {10.1109/86.808948},
  abstract = {The NavChair Assistive Wheelchair Navigation System is being developed to reduce the cognitive and physical requirements of operating a power wheelchair for people with wide ranging impairments that limit their access to powered mobility. The NavChair is based on a commercial wheelchair system with the addition of a DOS-based computer system, ultrasonic sensors, and an interface module interposed between the joystick and power module of the wheelchair. The obstacle avoidance routines used by the NavChair in conjunction with the ultrasonic sensors are modifications of methods originally used in mobile robotics research. The NavChair currently employs three operating modes: general obstacle avoidance, door passage, and automatic wall following. Results from performance testing of these three operating modes demonstrate their functionality. In additional to advancing the technology of smart wheelchairs, the NavChair has application to the development and testing of "shared control" systems where a human and machine share control of a system and the machine can automatically adapt to human behaviors.},
  keywords = {Wheelchairs},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/levineNavChairAssistiveWheelchair1999.pdf}
}

@article{linMicrosoftCOCOCommon2014,
  title = {Microsoft {{COCO}}: {{Common Objects}} in {{Context}}},
  author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Bourdev, Lubomir and Girshick, Ross and Hays, James and Perona, Pietro and Ramanan, Deva and Zitnick, C. Lawrence and Dollár, Piotr},
  date = {2014},
  doi = {10.48550/arXiv.1405.0312},
  abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
  keywords = {Dataset,Machine Learning},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/linMicrosoftCOCOCommon2014.pdf}
}

@article{liVisionBasedMobileIndoor2019,
  title = {Vision-{{Based Mobile Indoor Assistive Navigation Aid}} for {{Blind People}}},
  author = {Li, Bing and Munoz, Juan Pablo and Rong, Xuejian and Chen, Qingtian and Xiao, Jizhong and Tian, Yingli and Arditi, Aries and Yousuf, Mohammed},
  date = {2019},
  journaltitle = {IEEE transactions on mobile computing},
  shortjournal = {TMC},
  volume = {18},
  number = {3},
  pages = {702--714},
  publisher = {{IEEE}},
  location = {{United States}},
  issn = {1536-1233},
  doi = {10.1109/TMC.2018.2842751},
  abstract = {This paper presents a new holistic vision-based mobile assistive navigation system to help blind and visually impaired people with indoor independent travel. The system detects dynamic obstacles and adjusts path planning in real-time to improve navigation safety. First, we develop an indoor map editor to parse geometric information from architectural models and generate a semantic map consisting of a global 2D traversable grid map layer and context-aware layers. By leveraging the visual positioning service (VPS) within the Google Tango device, we design a map alignment algorithm to bridge the visual area description file (ADF) and semantic map to achieve semantic localization. Using the on-board RGB-D camera, we develop an efficient obstacle detection and avoidance approach based on a time-stamped map Kalman filter (TSM-KF) algorithm. A multi-modal human-machine interface (HMI) is designed with speech-audio interaction and robust haptic interaction through an electronic SmartCane. Finally, field experiments by blindfolded and blind subjects demonstrate that the proposed system provides an effective tool to help blind individuals with indoor navigation and wayfinding.},
  keywords = {Wheelchairs},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/liVisionBasedMobileIndoor2019.pdf}
}

@online{lucettiStereolabsZEDMini2018,
  title = {Stereolabs {{ZED Mini Holder}}},
  author = {Lucetti, Walter},
  date = {2018},
  url = {https://www.thingiverse.com/thing:3046034},
  keywords = {Hardware}
}

@article{lugaresiMediaPipeFrameworkBuilding2019,
  title = {{{MediaPipe}}: {{A Framework}} for {{Building Perception Pipelines}}},
  author = {Lugaresi, Camillo and Tang, Jiuqiang and Nash, Hadon and McClanahan, Chris and Uboweja, Esha and Hays, Michael and Zhang, Fan and Chang, Chuo-Ling and Yong, Ming Guang and Lee, Juhyun and Chang, Wan-Teh and Hua, Wei and Georg, Manfred and Grundmann, Matthias},
  date = {2019},
  doi = {10.48550/arXiv.1906.08172},
  abstract = {Building applications that perceive the world around them is challenging. A developer needs to (a) select and develop corresponding machine learning algorithms and models, (b) build a series of prototypes and demos, (c) balance resource consumption against the quality of the solutions, and finally (d) identify and mitigate problematic cases. The MediaPipe framework addresses all of these challenges. A developer can use MediaPipe to build prototypes by combining existing perception components, to advance them to polished cross-platform applications and measure system performance and resource consumption on target platforms. We show that these features enable a developer to focus on the algorithm or model development and use MediaPipe as an environment for iteratively improving their application with results reproducible across different devices and platforms. MediaPipe will be open-sourced at https://github.com/google/mediapipe.},
  keywords = {Framework,Machine Learning},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/lugaresiMediaPipeFrameworkBuilding2019.pdf}
}

@online{mathworksVectorFieldHistogram2022,
  type = {Documentation},
  title = {Vector {{Field Histogram}}},
  author = {{MathWorks}},
  date = {2022},
  url = {https://au.mathworks.com/help/nav/ug/vector-field-histograms.html},
  keywords = {Framework,Path Planning}
}

@online{microsoftAzureKinectDK2021,
  title = {Azure {{Kinect DK Hardware Specifications}}},
  author = {{Microsoft}},
  date = {2021},
  url = {https://docs.microsoft.com/en-us/azure/Kinect-dk/hardware-specification},
  keywords = {Hardware},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/microsoftAzureKinectDK2021.pdf}
}

@article{mnihHumanlevelControlDeep2015,
  title = {Human-Level Control through Deep Reinforcement Learning},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  date = {2015},
  journaltitle = {Nature (London)},
  shortjournal = {NATURE},
  volume = {518},
  number = {7540},
  pages = {529--533},
  publisher = {{Springer Nature}},
  location = {{LONDON}},
  issn = {0028-0836},
  doi = {10.1038/nature14236},
  abstract = {The theory of reinforcement learning provides a normative account', deeply rooted in psychological' and neuroscientifie perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory processing systems4'5, the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopaminergic neurons and temporal difference reinforcement learning algorithms'. While reinforcement learning agents have achieved some successes in a variety of domains", their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks'" to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games". We demonstrate that the deep Q-network agent, receiving only the pixels and the game score as inputs, was able to surpass the performance of all previous algorithms and achieve a level comparable to that of a professional human games tester across a set of 49 games, using the same algorithm, network architecture and hyperparameters. This work bridges the divide between high-dimensional sensory inputs and actions, resulting in the first artificial agent that is capable of learning to excel at a diverse array of challenging tasks.},
  keywords = {Machine Learning,Model},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/mnihHumanlevelControlDeep2015.pdf}
}

@article{mur-artalORBSLAM2OpenSourceSLAM2017,
  title = {{{ORB-SLAM2}}: {{An Open-Source SLAM System}} for {{Monocular}}, {{Stereo}}, and {{RGB-D Cameras}}},
  author = {Mur-Artal, Raul and Tardos, Juan D},
  date = {2017},
  journaltitle = {IEEE Transactions on Robotics},
  shortjournal = {TRO},
  volume = {33},
  number = {5},
  pages = {1255--1262},
  publisher = {{PISCATAWAY: IEEE}},
  location = {{PISCATAWAY}},
  issn = {1552-3098},
  doi = {10.1109/TRO.2017.2705103},
  abstract = {We present ORB-SLAM2, a complete simultaneous localization and mapping (SLAM) system for monocular, stereo and RGB-D cameras, including map reuse, loop closing, and relocalization capabilities. The system works in real time on standard central processing units in a wide variety of environments from small hand-held indoors sequences, to drones flying in industrial environments and cars driving around a city. Our back-end, based on bundle adjustment with monocular and stereo observations, allows for accurate trajectory estimation with metric scale. Our system includes a lightweight localization mode that leverages visual odometry tracks for unmapped regions and matches with map points that allow for zero-drift localization. The evaluation on 29 popular public sequences shows that our method achieves state-of-the-art accuracy, being in most cases the most accurate SLAM solution. We publish the source code, not only for the benefit of the SLAM community, but with the aim of being an out-of-the-box SLAM solution for researchers in other fields.},
  keywords = {Model,SLAM},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/mur-artalORBSLAM2OpenSourceSLAM2017.pdf}
}

@misc{nvidiaAmpereGA102GPU2020,
  title = {Ampere {{GA102 GPU Architecture}}},
  author = {{Nvidia}},
  date = {2020},
  url = {https://www.nvidia.com/content/PDF/nvidia-ampere-ga-102-gpu-architecture-whitepaper-v2.pdf},
  keywords = {Hardware},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/nvidiaAmpereGA102GPU2020.pdf}
}

@misc{nvidiaJetsonNanoSystemonModule2019,
  title = {Jetson {{Nano System-on-Module Data Sheet}}},
  author = {{Nvidia}},
  date = {2019},
  url = {https://developer.nvidia.com/embedded/downloads},
  keywords = {Hardware},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/nvidiaJetsonNanoSystemonModule2019.pdf}
}

@misc{nvidiaJetsonXavierNX2019,
  title = {Jetson {{Xavier NX Series System-on-Module Data Sheet}}},
  author = {{Nvidia}},
  date = {2019},
  url = {https://developer.nvidia.com/embedded/downloads},
  keywords = {Hardware},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/nvidiaJetsonXavierNX2019.pdf}
}

@misc{nvidiaTuringGPUArchitecture2018,
  title = {Turing {{GPU Architecture}}},
  author = {{Nvidia}},
  date = {2018},
  url = {https://images.nvidia.com/aem-dam/en-zz/Solutions/design-visualization/technologies/turing-architecture/NVIDIA-Turing-Architecture-Whitepaper.pdf},
  keywords = {Hardware},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/nvidiaTuringGPUArchitecture2018.pdf}
}

@inproceedings{p.viswanathanAdaptiveNavigationAssistance2011,
  title = {Adaptive {{Navigation Assistance}} for {{Visually-Impaired Wheelchair Users}}},
  booktitle = {New and {{Emerging Technologies}} in {{Assistive Robotics}}},
  author = {{P. Viswanathan} and {J. J. Little} and {A. K. Mackworth} and {A. Mihailidis}},
  date = {2011-09-26},
  url = {https://www.cs.ubc.ca/~mack/Publications/ViswanathanIROSWorkshop2011.pdf},
  abstract = {It is estimated that approximately 10\% of people who are legally blind require wheelchairs [1]. Wheelchair users with visual impairments face difficulties in avoiding obstacles as well as identifying visual cues in the environment, thus making independent navigation challenging, and in some cases, impossible. The authors in [1] suggest that intelligent wheelchairs capable of collision avoidance and path planning would greatly benefit wheelchair users with visual impairment. Although several intelligent wheelchairs have been developed recently [2-4], these wheelchairs navigate autonomously, thus taking control away from the user. On the other hand, wheelchairs that only provide collision avoidance support [5] are not appropriate for drivers who are unable to determine their location and want to navigate to a specific location. We thus present a novel, real-time, vision-based intelligent wheelchair system that avoids collisions and provides adaptive audio prompts to help blindfolded users navigate to specified destinations. Existing intelligent wheelchairs have used various active sensors (acoustic, sonar, infrared, laser, etc.) [6]. We rely solely on a stereovision camera due to its low power consumption, ability to perform in natural environments, and relatively low cost. Most outdoor wayfinding systems rely on GPS, which is unreliable in indoor settings, while indoor wayfinding systems typically use beacon and RFID technology, which require modifications to the environment. By using vision-based techniques we can achieve accurate localization, while reducing/eliminating the need for environment modifications. In addition, cameras capture and provide a richer dataset than can be used for high-level scene understanding to build maps and determine what type of room the wheelchair is in.},
  eventtitle = {{{IROS}} 2011},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/viswanathanAdaptiveNavigationAssistance2011.pdf}
}

@article{paszkePyTorchImperativeStyle2019,
  title = {{{PyTorch}}: {{An Imperative Style}}, {{High-Performance Deep Learning Library}}},
  author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Köpf, Andreas and Yang, Edward and DeVito, Zach and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  date = {2019},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {32},
  pages = {8024--8035},
  abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.},
  keywords = {Framework,Machine Learning},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/paszkePyTorchImperativeStyle2019.pdf}
}

@article{redmonYOLO9000BetterFaster2016,
  title = {{{YOLO9000}}: {{Better}}, {{Faster}}, {{Stronger}}},
  author = {Redmon, Joseph and Farhadi, Ali},
  date = {2016},
  doi = {10.48550/arXiv.1612.08242},
  abstract = {We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster RCNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on object detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that don't have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. But YOLO can detect more than just 200 classes},
  keywords = {Machine Learning,Model},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/redmonYOLO9000BetterFaster2016.pdf}
}

@article{redmonYOLOv3IncrementalImprovement2018,
  title = {{{YOLOv3}}: {{An Incremental Improvement}}},
  author = {Redmon, Joseph and Farhadi, Ali},
  date = {2018},
  doi = {10.48550/arXiv.1804.02767},
  abstract = {We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network that's pretty swell. It's a little bigger than last time but more accurate. It's still fast though, don't worry. At 320x320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. When we look at the old .5 IOU mAP detection metric YOLOv3 is quite good. It achieves 57.9 mAP@50 in 51 ms on a Titan X, compared to 57.5 mAP@50 in 198 ms by RetinaNet, similar performance but 3.8x faster. As always, all the code is online at https://pjreddie.com/yolo/},
  keywords = {Machine Learning,Model},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/redmonYOLOv3IncrementalImprovement2018.pdf}
}

@article{redmonYouOnlyLook2015,
  title = {You {{Only Look Once}}: {{Unified}}, {{Real-Time Object Detection}}},
  author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  date = {2015},
  doi = {10.48550/arXiv.1506.02640},
  abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
  keywords = {Machine Learning,Model},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/redmonYouOnlyLook2015.pdf}
}

@article{renFasterRCNNRealTime2015,
  title = {Faster {{R-CNN}}: {{Towards Real-Time Object Detection}} with {{Region Proposal Networks}}},
  author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  date = {2015},
  doi = {10.48550/ARXIV.1506.01497},
  abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
  keywords = {Machine Learning,Model},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/renFasterRCNNRealTime2015.pdf}
}

@inproceedings{rocklandVoiceActivatedWheelchair1998,
  title = {Voice Activated Wheelchair Controller},
  booktitle = {Proceedings of the {{IEEE}} 24th {{Annual Northeast Bioengineering Conference}}},
  author = {Rockland, R.H and Reisman, S},
  date = {1998},
  pages = {128--129},
  publisher = {{IEEE}},
  doi = {10.1109/NEBC.1998.664900},
  abstract = {The Voice Activated Wheelchair Controller (VAWC) project was designed to develop a feasibility model for activating a wheelchair using a low-cost speech recognition system. A microcontroller was programmed to provide user control over each command, as well as to prevent voice commands from being issued accidentally.},
  isbn = {978-0-7803-4544-7},
  keywords = {Wheelchairs},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/rocklandVoiceActivatedWheelchair1998.pdf}
}

@article{ronnebergerUNetConvolutionalNetworks2015,
  title = {U-{{Net}}: {{Convolutional Networks}} for {{Biomedical Image Segmentation}}},
  author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  date = {2015},
  doi = {10.48550/ARXIV.1505.04597},
  abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
  keywords = {Machine Learning,Model},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/ronnebergerUNetConvolutionalNetworks2015.pdf}
}

@article{ryuDevelopmentAutonomousDriving2022,
  title = {Development of an {{Autonomous Driving Smart Wheelchair}} for the {{Physically Weak}}},
  author = {Ryu, Hye-Yeon and Kwon, Je-Seong and Lim, Jeong-Hak and Kim, A-Hyeon and Baek, Su-Jin and Kim, Jong-Wook},
  date = {2022},
  journaltitle = {Applied sciences},
  volume = {12},
  number = {1},
  pages = {377},
  publisher = {{MDPI AG}},
  issn = {2076-3417},
  doi = {10.3390/app12010377},
  abstract = {People who have difficulty moving owing to problems in walking spend their lives assisted by wheelchairs. In the past, research has been conducted regarding the application of various technologies to electric wheelchairs for user convenience. In this study, we evaluated a method of applying an autonomous driving function and developed an autonomous driving function using ROS. An electric wheelchair with a control unit designed to enable autonomous driving was used to test the basic performance of autonomous driving. The effectiveness of the technology was confirmed by comparing the results of autonomous driving with those of manual driving on the same route. It is expected that the evaluation and improvement of the usability and ride quality as well as additional studies will help improve the mobility convenience of physically disabled persons.},
  keywords = {Wheelchairs},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/ryuDevelopmentAutonomousDriving2022.pdf}
}

@article{sakaiPythonRoboticsPythonCode2018,
  title = {{{PythonRobotics}}: A {{Python}} Code Collection of Robotics Algorithms},
  author = {Sakai, Atsushi and Ingram, Daniel and Dinius, Joseph and Chawla, Karan and Raffin, Antonin and Paques, Alexis},
  date = {2018},
  doi = {10.48550/ARXIV.1808.10703},
  abstract = {This paper describes an Open Source Software (OSS) project: PythonRobotics. This is a collection of robotics algorithms implemented in the Python programming language. The focus of the project is on autonomous navigation, and the goal is for beginners in robotics to understand the basic ideas behind each algorithm. In this project, the algorithms which are practical and widely used in both academia and industry are selected. Each sample code is written in Python3 and only depends on some standard modules for readability and ease of use. It includes intuitive animations to understand the behavior of the simulation.},
  keywords = {Framework,Path Planning},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/sakaiPythonRoboticsPythonCode2018.pdf}
}

@article{scudellariSelfdrivingWheelchairsDebut2017,
  title = {Self-Driving Wheelchairs Debut in Hospitals and Airports [{{News}}]},
  author = {Scudellari, Megan},
  date = {2017},
  journaltitle = {IEEE Spectrum},
  shortjournal = {SPEC},
  volume = {54},
  number = {10},
  pages = {14},
  publisher = {{PISCATAWAY: IEEE}},
  location = {{PISCATAWAY}},
  issn = {0018-9235},
  doi = {10.1109/MSPEC.2017.8048827},
  abstract = {Autonomous vehicles can add a new member to their ranks-the self-driving wheelchair. This summer, two robotic wheelchairs made headlines: one at a Singaporean hospital and another at a Japanese airport. The Singapore-MIT Alliance for Research and Technology, or SMART, developed the former, first deployed in Singapore's Changi General Hospital in September 2016, where it successfully navigated the hospital's hallways. It is the latest in a string of autonomous vehicles made by SMART, including a golf cart, an electric taxi, and most recently, a scooter that zipped more than 100 MIT visitors around on tours in 2016.},
  keywords = {News,Wheelchairs},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/scudellariSelfdrivingWheelchairsDebut2017.pdf}
}

@article{simonyanVeryDeepConvolutional2014,
  title = {Very {{Deep Convolutional Networks}} for {{Large-Scale Image Recognition}}},
  author = {Simonyan, Karen and Zisserman, Andrew},
  date = {2014},
  doi = {10.48550/ARXIV.1409.1556},
  abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
  keywords = {Machine Learning,Model},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/simonyanVeryDeepConvolutional2014.pdf}
}

@article{simpsonHowManyPeople2008,
  title = {How Many People Would Benefit from a Smart Wheelchair?},
  author = {Simpson, Richard C and LoPresti, Edmund F and Cooper, Rory A},
  date = {2008},
  journaltitle = {Journal of rehabilitation research and development},
  shortjournal = {J REHABIL RES DEV},
  volume = {45},
  number = {1},
  pages = {53--72},
  publisher = {{BALTIMORE: JOURNAL REHAB RES \& DEV}},
  location = {{BALTIMORE}},
  issn = {0748-7711},
  doi = {10.1682/JRRD.2007.01.0015},
  abstract = {Independent mobility is important, but some wheelchair users find operating existing manual or powered wheelchairs difficult or impossible. Challenges to safe, independent wheelchair use can result from various overlapping physical, perceptual, or cognitive symptoms of diagnoses such as spinal cord injury, cerebrovascular accident, multiple sclerosis, amyotrophic lateral sclerosis, and cerebral palsy. Persons with different symptom combinations can benefit from different types of assistance from a smart wheelchair and different wheelchair form factors. The sizes of these user populations have been estimated based on published estimates of the number of individuals with each of several diseases who (1) also need a wheeled mobility device and (2) have specific symptoms that could interfere with mobility device use.},
  keywords = {Wheelchairs},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/simpsonHowManyPeople2008.pdf}
}

@misc{stereolabsZEDCameraSDK2019,
  title = {{{ZED}} 2 {{Camera}} and {{SDK Overview}}},
  author = {{Stereolabs}},
  date = {2019},
  url = {https://cdn.stereolabs.com/assets/datasheets/zed2-camera-datasheet.pdf},
  keywords = {Hardware},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/stereolabsZEDCameraSDK2019.pdf}
}

@misc{stereolabsZEDMiniCamera2018,
  title = {{{ZED Mini Camera}} and {{SDK Overview}}},
  author = {{Stereolabs}},
  date = {2018},
  url = {https://cdn.stereolabs.com/assets/datasheets/zed-mini-camera-datasheet.pdf},
  keywords = {Hardware},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/stereolabsZEDMiniCamera2018.pdf}
}

@article{szegedyGoingDeeperConvolutions2014,
  title = {Going {{Deeper}} with {{Convolutions}}},
  author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  date = {2014},
  doi = {10.48550/ARXIV.1409.4842},
  abstract = {We propose a deep convolutional neural network architecture codenamed "Inception", which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
  keywords = {Machine Learning,Model},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/szegedyGoingDeeperConvolutions2014.pdf}
}

@article{taketomiVisualSLAMAlgorithms2017,
  title = {Visual {{SLAM}} Algorithms: A Survey from 2010 to 2016},
  author = {Taketomi, Takafumi and Uchiyama, Hideaki and Ikeda, Sei},
  date = {2017},
  journaltitle = {IPSJ Transactions on Computer Vision and Applications},
  shortjournal = {IPSJ T Comput Vis Appl},
  volume = {9},
  number = {1},
  pages = {1--11},
  publisher = {{Berlin/Heidelberg: Springer Berlin Heidelberg}},
  location = {{Berlin/Heidelberg}},
  issn = {1882-6695},
  doi = {10.1186/s41074-017-0027-2},
  abstract = {SLAM is an abbreviation for simultaneous localization and mapping, which is a technique for estimating sensor motion and reconstructing structure in an unknown environment. Especially, Simultaneous Localization and Mapping (SLAM) using cameras is referred to as visual SLAM (vSLAM) because it is based on visual information only. vSLAM can be used as a fundamental technology for various types of applications and has been discussed in the field of computer vision, augmented reality, and robotics in the literature. This paper aims to categorize and summarize recent vSLAM algorithms proposed in different research communities from both technical and historical points of views. Especially, we focus on vSLAM algorithms proposed mainly from 2010 to 2016 because major advance occurred in that period. The technical categories are summarized as follows: feature-based, direct, and RGB-D camera-based approaches.},
  keywords = {Recommended,Survey},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/VSLAM Survey.pdf}
}

@article{tanEfficientNetRethinkingModel2019,
  title = {{{EfficientNet}}: {{Rethinking Model Scaling}} for {{Convolutional Neural Networks}}},
  author = {Tan, Mingxing and Le, Quoc V},
  date = {2019},
  doi = {10.48550/ARXIV.1905.11946},
  abstract = {International Conference on Machine Learning, 2019 Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.3\% top-1 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7\%), Flowers (98.8\%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.},
  keywords = {Machine Learning,Model},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/tanEfficientNetRethinkingModel2019.pdf}
}

@incollection{thrunRoboticMappingSurvey2002,
  title = {Robotic {{Mapping}}: {{A Survey}}},
  booktitle = {Exploring {{Artificial Intelligence}} in the {{New Millenium}}},
  author = {Thrun, Sebastian},
  date = {2002},
  publisher = {{Morgan Kaufmann}},
  url = {robots.stanford.edu/papers/thrun.mapping-tr.html},
  abstract = {This article provides a comprehensive introduction into the field of robotic mapping, with a focus on indoor mapping. It describes and compares various probabilistic techniques, as they are presently being applied to a vast array of mobile robot mapping problems. The history of robotic mapping is also described, along with an extensive list of open research problems.},
  keywords = {Recommended,Survey},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/EFK Survey.pdf}
}

@inproceedings{tomariEnhancingWheelchairControl2014,
  title = {Enhancing {{Wheelchair}}’s {{Control Operation}} of a {{Severe Impairment User}}},
  booktitle = {The 8th {{International Conference}} on {{Robotic}}, {{Vision}}, {{Signal Processing}} \& {{Power Applications}}},
  author = {Tomari, Mohd Razali Md and Kobayashi, Yoshinori and Kuno, Yoshinori},
  editor = {Mat Sakim, Harsa Amylia and Mustaffa, Mohd Tafir},
  date = {2014},
  pages = {65--72},
  publisher = {{Springer Singapore}},
  location = {{Singapore}},
  doi = {10.1007/978-981-4585-42-2},
  abstract = {Users with severe motor ability are unable to control their wheelchair using standard joystick and hence an alternative control input is preferred. However, using such an input, undoubtedly the navigation burden for the user is significantly increased. In this paper a method on how to reduce such a burden with the help of smart navigation platform is proposed. Initially, user information is inferred using an IMU sensor and a bite-like switch. Then information from the environment is obtained using combination of laser and Kinect sensors. Eventually, both information from the environment and the user is analyzed to decide the final control operation that according to the user intention, safe and comfortable to the people in the surrounding. Experimental results demonstrate the feasibility of the proposed approach.},
  isbn = {978-981-4585-42-2},
  keywords = {From Survey,Wheelchairs},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/tomariEnhancingWheelchairControl2014.pdf}
}

@article{tomariEnhancingWheelchairManoeuvrability2013,
  title = {Enhancing {{Wheelchair Manoeuvrability}} for {{Severe Impairment Users}}},
  author = {Tomari, Razali and Kobayashi, Yoshinori and Kuno, Yoshinori},
  date = {2013},
  journaltitle = {International Journal of Advanced Robotic Systems},
  shortjournal = {INT J ADV ROBOT SYST},
  volume = {10},
  number = {2},
  pages = {92},
  publisher = {{London, England: SAGE Publications}},
  location = {{London, England}},
  issn = {1729-8806},
  doi = {10.5772/55477},
  abstract = {A significant number of individuals with severe motor impairments are unable to control their wheelchair using a standard joystick. Even when they can facilitate the control input, navigation in a confined space or crowded environments is still a great challenge. Here we propose a wheelchair framework that enables the user to issue the command via a multi-input hands free interface (HFI), which subsequently assists him/her to overcome difficult circumstances using a multimodal control strategy. Initially the HFI inputs are analysed to infer the desired control mode and the user command. Then environmental information is perceived using a combination of laser and Kinect sensors for determining all possible obstacle locations and outputting a safety map around the wheelchair's vicinity. Eventually, the user's command is validated with the safety map to moderate the final motion, which is collision free and the best for the user's preference. The proposed method can reduce the burden of severe impairment users when controlling wheelchairs by continuously monitoring the surroundings and can make them move easily according to the users' intention.},
  keywords = {Wheelchairs},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/tomariEnhancingWheelchairManoeuvrability2013.pdf}
}

@article{treflerOutcomesWheelchairSystems2004,
  title = {Outcomes of {{Wheelchair Systems Intervention With Residents}} of {{Long-Term Care Facilities}}},
  author = {Trefler, Elaine and Fitzgerald, Shirley G. and Hobson, Douglas A. and Bursick, Thomas and Joseph, Robert},
  date = {2004},
  journaltitle = {Assistive technology},
  shortjournal = {ASSIST TECHNOL},
  volume = {16},
  number = {1},
  pages = {18--27},
  publisher = {{Taylor \& Francis Group}},
  location = {{ARLINGTON}},
  issn = {1040-0435},
  doi = {10.1080/10400435.2004.10132071},
  abstract = {This pilot study was designed to measure the effects of individually prescribed wheelchair systems on posture and reach, mobility, quality of life, and satisfaction with technology for residents of long-term care facilities. Thirty persons 60 years of age or older who resided permanently in a long-term care facility and who used seating and mobility systems for 6 hours or more each day were recruited for this project. Outcomes included timed independent mobility, forward and lateral reach, quality of life, and satisfaction with assistive technology. The study used semicrossover design with participants measured three times. Measurements were first made in the existing seating and mobility system and a second time immediately after participants were provided with individually prescribed seating and mobility systems. The final measurement was 3 months after the delivery of the individually prescribed system. Results indicated that individually fitted wheelchair systems for elderly residents of long-term care facilities are beneficial. Participants had less difficulty independently propelling their systems and increased forward reach, quality of life, and satisfaction with assistive technology. The study used semicrossover design with participants measured three times. Measurements were first made in the existing seating and mobility system and a second time immediately after participants were provided with individually prescribed seating and mobility systems. The final measurement was 3 months after the delivery of the individually prescribed system. Results indicated that individually fitted wheelchair systems for elderly residents of long-term care facilities are beneficial. Participants had less difficulty independently propelling their systems and increased forward reach, quality of life for social function and physical role, and satisfaction with the new wheelchair technology. Persons residing in extended care facilities benefit from receiving individually prescribed wheelchair systems. The individual systems enhance elderly persons' independent mobility, functional reach, feeling of well-being, and satisfaction with their assistive technology.},
  keywords = {Wheelchairs},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/treflerOutcomesWheelchairSystems2004.pdf}
}

@article{ulrichVFHReliableObstacle1998,
  title = {{{VFH}}+: {{Reliable Obstacle Avoidance}} for {{Fast Mobile Robots}}},
  author = {Ulrich, I and Borenstein, J},
  date = {1998},
  journaltitle = {IEEE International Conference on Robotics and Automation},
  shortjournal = {ROBOT},
  volume = {2},
  pages = {1572--1577},
  issn = {1050-4729},
  doi = {10.1109/ROBOT.1998.677362},
  abstract = {This paper presents further improvements on the earlier vector field histogram (VFH) method developed by Borenstein-Koren (1991) for real-time mobile robot obstacle avoidance. The enhanced method, called VFH+, offers several improvements that result in smoother robot trajectories and greater reliability. VFH+ reduces some of the parameter tuning of the original VFH method by explicitly compensating for the robot width. Also added in VFH+ is a better approximation of the mobile robot trajectory, which results in higher reliability.},
  keywords = {Path Planning},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/ulrichVFHReliableObstacle1998.pdf}
}

@software{ultralyticsYOLOv5,
  title = {{{YOLOv5}}},
  author = {{Ultralytics}},
  url = {https://github.com/ultralytics/yolov5},
  keywords = {Machine Learning,Model}
}

@article{vanderpoortenPoweredWheelchairNavigation2012,
  title = {Powered Wheelchair Navigation Assistance through Kinematically Correct Environmental Haptic Feedback},
  author = {Vander Poorten, E. B and Demeester, E and Reekmans, E and Philips, J and Huntemann, A and De Schutter, J},
  date = {2012},
  journaltitle = {IEEE International Conference on Robotics and Automation},
  shortjournal = {ICRA},
  pages = {3706--3712},
  issn = {1050-4729},
  doi = {10.1109/ICRA.2012.6225349},
  abstract = {This article introduces a set of novel haptic guidance algorithms intended to provide intuitive and reliable assistance for electric wheelchair navigation through narrow or crowded spaces. The proposed schemes take hereto the non-holonomic nature and a detailed geometry of the wheelchair into consideration. The methods encode the environment as a set of collision-free circular paths and, making use of a model-free impedance controller, `haptically' guide the user along collision-free paths or away from obstructed paths or paths that simply do not coincide with the motion intended by the user. The haptic feedback plays a central role as it establishes a fast bilateral communication channel between user and wheelchair controller and allows a direct negotiation about wheelchair motion. If found unsatisfactory, suggested trajectories can always be overruled by the user. Relying on inputs from user modeling and intention recognition schemes, the system can reduce forces needed to move along intended directions, thereby avoiding unnecessary fatigue of the user. A commercial powered wheelchair was upgraded and feasability tests were conducted to validate the proposed methods. The potential of the proposed approaches was hereby demonstrated.},
  keywords = {From Survey,Wheelchairs},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/vanderpoortenPoweredWheelchairNavigation2012.pdf}
}

@article{vaswaniAttentionAllYou2017,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
  date = {2017},
  doi = {10.48550/arXiv.1706.03762},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  keywords = {Machine Learning,Model},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/vaswaniAttentionAllYou2017.pdf}
}

@article{vuHybridNetsEndtoEndPerception2022,
  title = {{{HybridNets}}: {{End-to-End Perception Network}}},
  author = {Vu, Dat and Ngo, Bao and Phan, Hung},
  date = {2022},
  doi = {10.48550/ARXIV.2203.09035},
  abstract = {End-to-end Network has become increasingly important in multi-tasking. One prominent example of this is the growing significance of a driving perception system in autonomous driving. This paper systematically studies an end-to-end perception network for multi-tasking and proposes several key optimizations to improve accuracy. First, the paper proposes efficient segmentation head and box/class prediction networks based on weighted bidirectional feature network. Second, the paper proposes automatically customized anchor for each level in the weighted bidirectional feature network. Third, the paper proposes an efficient training loss function and training strategy to balance and optimize network. Based on these optimizations, we have developed an end-to-end perception network to perform multi-tasking, including traffic object detection, drivable area segmentation and lane detection simultaneously, called HybridNets, which achieves better accuracy than prior art. In particular, HybridNets achieves 77.3 mean Average Precision on Berkeley DeepDrive Dataset, outperforms lane detection with 31.6 mean Intersection Over Union with 12.83 million parameters and 15.6 billion floating-point operations. In addition, it can perform visual perception tasks in real-time and thus is a practical and accurate solution to the multi-tasking problem. Code is available at https://github.com/datvuthanh/HybridNets.},
  keywords = {Machine Learning,Model},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/vuHybridNetsEndtoEndPerception2022.pdf}
}

@article{wangS2P2SelfSupervisedGoalDirected2021,
  title = {{{S2P2}}: {{Self-Supervised Goal-Directed Path Planning Using RGB-D Data}} for {{Robotic Wheelchairs}}},
  author = {Wang, Hengli and Sun, Yuxiang and Fan, Rui and Liu, Ming},
  date = {2021},
  journaltitle = {IEEE International Conference on Robotics and Automation (ICRA)},
  pages = {11422--11428},
  doi = {10.1109/ICRA48506.2021.9561314},
  url = {https://sites.google.com/view/s2p2},
  abstract = {Path planning is a fundamental capability for autonomous navigation of robotic wheelchairs. With the impressive development of deep-learning technologies, imitation learning-based path planning approaches have achieved effective results in recent years. However, the disadvantages of these approaches are twofold: 1) they may need extensive time and labor to record expert demonstrations as training data},
  keywords = {Wheelchairs},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/wangS2P2SelfSupervisedGoalDirected2021.pdf}
}

@article{wangSelfSupervisedDrivableArea2019,
  title = {Self-{{Supervised Drivable Area}} and {{Road Anomaly Segmentation Using RGB-D Data For Robotic Wheelchairs}}},
  author = {Wang, Hengli and Sun, Yuxiang and Liu, Ming},
  date = {2019},
  journaltitle = {IEEE Robotics and Automation Letters},
  shortjournal = {LRA},
  volume = {4},
  number = {4},
  pages = {4386--4393},
  publisher = {{IEEE}},
  issn = {2377-3766},
  doi = {10.1109/LRA.2019.2932874},
  abstract = {The segmentation of drivable areas and road anomalies are critical capabilities to achieve autonomous navigation for robotic wheelchairs. The recent progress of semantic segmentation using deep learning techniques has presented effective results. However, the acquisition of large-scale datasets with hand-labeled ground truth is time-consuming and labor-intensive, making the deep learning-based methods often hard to implement in practice. We contribute to the solution of this problem for the task of drivable area and road anomaly segmentation by proposing a self-supervised learning approach. We develop a pipeline that can automatically generate segmentation labels for drivable areas and road anomalies. Then, we train RGB-D data-based semantic segmentation neural networks and get predicted labels. Experimental results show that our proposed automatic labeling pipeline achieves an impressive speed-up compared to manual labeling. In addition, our proposed self-supervised approach exhibits more robust and accurate results than the state-of-the-art traditional algorithms as well as the state-of-the-art self-supervised algorithms.},
  keywords = {Wheelchairs},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/wangSelfSupervisedDrivableArea2019.pdf}
}

@inproceedings{wanUnscentedKalmanFilter2000,
  title = {The Unscented {{Kalman}} Filter for Nonlinear Estimation},
  booktitle = {Proceedings of the {{IEEE}} 2000 {{Adaptive Systems}} for {{Signal Processing}}, {{Communications}}, and {{Control Symposium}}},
  author = {Wan, E.A and Van Der Merwe, R},
  date = {2000},
  pages = {153--158},
  publisher = {{IEEE}},
  doi = {10.1109/ASSPCC.2000.882463},
  abstract = {This paper points out the flaws in using the extended Kalman filter (EKE) and introduces an improvement, the unscented Kalman filter (UKF), proposed by Julier and Uhlman (1997). A central and vital operation performed in the Kalman filter is the propagation of a Gaussian random variable (GRV) through the system dynamics. In the EKF the state distribution is approximated by a GRV, which is then propagated analytically through the first-order linearization of the nonlinear system. This can introduce large errors in the true posterior mean and covariance of the transformed GRV, which may lead to sub-optimal performance and sometimes divergence of the filter. The UKF addresses this problem by using a deterministic sampling approach. The state distribution is again approximated by a GRV, but is now represented using a minimal set of carefully chosen sample points. These sample points completely capture the true mean and covariance of the GRV, and when propagated through the true nonlinear system, captures the posterior mean and covariance accurately to the 3rd order (Taylor series expansion) for any nonlinearity. The EKF in contrast, only achieves first-order accuracy. Remarkably, the computational complexity of the UKF is the same order as that of the EKF. Julier and Uhlman demonstrated the substantial performance gains of the UKF in the context of state-estimation for nonlinear control. Machine learning problems were not considered. We extend the use of the UKF to a broader class of nonlinear estimation problems, including nonlinear system identification, training of neural networks, and dual estimation problems. In this paper, the algorithms are further developed and illustrated with a number of additional examples.},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/wanUnscentedKalmanFilter2000.pdf}
}

@article{weiMmWaveRadarVision2022,
  title = {{{MmWave Radar}} and {{Vision Fusion}} for {{Object Detection}} in {{Autonomous Driving}}: {{A Review}}},
  author = {Wei, Zhiqing and Zhang, Fengkai and Chang, Shuo and Liu, Yangyang and Wu, Huici and Feng, Zhiyong},
  date = {2022},
  journaltitle = {Sensors},
  volume = {22},
  number = {7},
  issn = {1424-8220},
  doi = {10.3390/s22072542},
  abstract = {With autonomous driving developing in a booming stage, accurate object detection in complex scenarios attract wide attention to ensure the safety of autonomous driving. Millimeter wave (mmWave) radar and vision fusion is a mainstream solution for accurate obstacle detection. This article presents a detailed survey on mmWave radar and vision fusion based obstacle detection methods. First, we introduce the tasks, evaluation criteria, and datasets of object detection for autonomous driving. The process of mmWave radar and vision fusion is then divided into three parts: sensor deployment, sensor calibration, and sensor fusion, which are reviewed comprehensively. Specifically, we classify the fusion methods into data level, decision level, and feature level fusion methods. In addition, we introduce three-dimensional(3D) object detection, the fusion of lidar and vision in autonomous driving and multimodal information fusion, which are promising for the future. Finally, we summarize this article.},
  keywords = {Cars,Survey},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/weiMmWaveRadarVision2022.pdf}
}

@article{werlingOptimalTrajectoryGeneration2010,
  title = {Optimal {{Trajectory Generation}} for {{Dynamic Street Scenarios}} in a {{Frenét Frame}}},
  author = {Werling, Moritz and Ziegler, Julius and Kammel, Sören and Thrun, Sebastian},
  date = {2010},
  journaltitle = {IEEE International Conference on Robotics and Automation},
  shortjournal = {ROBOT},
  pages = {987--993},
  issn = {1050-4729},
  doi = {10.1109/ROBOT.2010.5509799},
  abstract = {Safe handling of dynamic highway and inner city scenarios with autonomous vehicles involves the problem of generating traffic-adapted trajectories. In order to account for the practical requirements of the holistic autonomous system, we propose a semi-reactive trajectory generation method, which can be tightly integrated into the behavioral layer. The method realizes long-term objectives such as velocity keeping, merging, following, stopping, in combination with a reactive collision avoidance by means of optimal-control strategies within the Frenét-Frame of the street. The capabilities of this approach are demonstrated in the simulation of a typical high-speed highway scenario.},
  keywords = {Path Planning},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/werlingOptimalTrajectoryGeneration2010.pdf}
}

@article{wuYOLOPYouOnly2021,
  title = {{{YOLOP}}: {{You Only Look Once}} for {{Panoptic Driving Perception}}},
  author = {Wu, Dong and Liao, Manwen and Zhang, Weitian and Wang, Xinggang and Bai, Xiang and Cheng, Wenqing and Liu, Wenyu},
  date = {2021},
  doi = {10.48550/ARXIV.2108.11250},
  abstract = {A panoptic driving perception system is an essential part of autonomous driving. A high-precision and real-time perception system can assist the vehicle in making the reasonable decision while driving. We present a panoptic driving perception network (YOLOP) to perform traffic object detection, drivable area segmentation and lane detection simultaneously. It is composed of one encoder for feature extraction and three decoders to handle the specific tasks. Our model performs extremely well on the challenging BDD100K dataset, achieving state-of-the-art on all three tasks in terms of accuracy and speed. Besides, we verify the effectiveness of our multi-task learning model for joint training via ablative studies. To our best knowledge, this is the first work that can process these three visual perception tasks simultaneously in real-time on an embedded device Jetson TX2(23 FPS) and maintain excellent accuracy. To facilitate further research, the source codes and pre-trained models are released at https://github.com/hustvl/YOLOP.},
  keywords = {Machine Learning,Model},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/wuYOLOPYouOnly2021.pdf}
}

@article{yuBDD100KDiverseDriving2018,
  title = {{{BDD100K}}: {{A Diverse Driving Dataset}} for {{Heterogeneous Multitask Learning}}},
  author = {Yu, Fisher and Chen, Haofeng and Wang, Xin and Xian, Wenqi and Chen, Yingying and Liu, Fangchen and Madhavan, Vashisht and Darrell, Trevor},
  date = {2018},
  journaltitle = {arXiv},
  doi = {10.48550/ARXIV.1805.04687},
  abstract = {Datasets drive vision progress, yet existing driving datasets are impoverished in terms of visual content and supported tasks to study multitask learning for autonomous driving. Researchers are usually constrained to study a small set of problems on one dataset, while real-world computer vision applications require performing tasks of various complexities. We construct BDD100K, the largest driving video dataset with 100K videos and 10 tasks to evaluate the exciting progress of image recognition algorithms on autonomous driving. The dataset possesses geographic, environmental, and weather diversity, which is useful for training models that are less likely to be surprised by new conditions. Based on this diverse dataset, we build a benchmark for heterogeneous multitask learning and study how to solve the tasks together. Our experiments show that special training strategies are needed for existing models to perform such heterogeneous tasks. BDD100K opens the door for future studies in this important venue.},
  keywords = {Dataset,Machine Learning},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/yuBDD100KDiverseDriving2018.pdf}
}

@inproceedings{yuLaneDepartureWarning2008,
  title = {A {{Lane Departure Warning System Based}} on {{Machine Vision}}},
  booktitle = {2008 {{IEEE Pacific-Asia Workshop}} on {{Computational Intelligence}} and {{Industrial Application}}},
  author = {Yu, Bing and Zhang, Weigong and Cai, Yingfeng},
  date = {2008},
  volume = {1},
  pages = {197--201},
  publisher = {{IEEE}},
  doi = {10.1109/PACIIA.2008.142},
  abstract = {Lane departure warning system based on machine vision is a human decision-make like solution to avoid lane departure fatalities with low cost and high reliability. In this paper, the model of vision-based lane departure warning system and the realization is described at first. Then the method of lane detection is illustrated, which is composed of three steps: image preprocessing, binary processing and dynamical threshold choosing, and linear-parabolic model fitting. After that, the solution of how to perform the departure decision-making is proposed and demonstrated. Unlike the usual TLC (Woong Kwon et al., 1999) and CCP (Risack et al., 2000) methods, the angles between lanes and the horizontal axis in captured image coordinate are used as the criterion for lane departure decision-making. At last the experiments are implemented by use of all the steps},
  keywords = {Cars},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/yuLaneDepartureWarning2008.pdf}
}


