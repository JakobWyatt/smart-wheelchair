\documentclass[10pt,twoside]{article}

\usepackage{setspace} \singlespacing % line spacing
\usepackage[top=2.5cm,bottom=2.5cm,inner=4cm,outer=2.5cm]{geometry} % margin sizes
\usepackage[none]{hyphenat} % dont hyphenate words
\usepackage{fontspec} \setmainfont{Times New Roman} % set font
\usepackage{microtype} % fix typing
\usepackage{amsmath} % equations
\usepackage{graphicx} % images
\usepackage{float} % figure positioning
\usepackage[hidelinks]{hyperref} % dont highlight links
\usepackage{cleveref} % easy to reference figures
\usepackage{siunitx} % format si units
\usepackage{changepage} % for the adjustwidth environment
\usepackage[authordate,backend=biber]{biblatex-chicago} \addbibresource{../zotero.bib} % referencing
\usepackage[export]{adjustbox} % frame figure
\usepackage{booktabs} % rules in table
\usepackage{subcaption} % subfigures

\raggedbottom % allow pages to end early
\pdfvariable minorversion 7 % updated pdf version
\setlength\parindent{5mm} % change indent size
\pagenumbering{gobble} % no page numbers
\AtEveryBibitem{\clearfield{title}} % no article titles
\AtEveryBibitem{\clearfield{url}} % no url
\setlength\bibitemsep{0pt} % change separation between bib items
% Fix references before submission

% change section/subsection title font size to 10
\usepackage{sectsty}
\sectionfont{\fontsize{10}{10}\selectfont}
\subsectionfont{\fontsize{10}{10}\selectfont}
\subsubsectionfont{\fontsize{10}{10}\selectfont}

% change spacing before & after headings
\usepackage{titlesec}
\titlespacing*{\section}{0pt}{20pt}{6pt} % left, before, after
\titlespacing*{\subsection}{0pt}{20pt}{6pt}
\titlespacing*{\subsubsection}{0pt}{20pt}{6pt}

% change spacing around captions and equations
% indent equations
\setlength{\abovecaptionskip}{10pt}
\setlength{\abovedisplayskip}{10pt}
\setlength{\belowdisplayskip}{10pt}
\setlength{\abovedisplayshortskip}{10pt}
\setlength{\belowdisplayshortskip}{10pt}

\begin{document}

\begin{center}
\fontsize{12}{14.4}\selectfont
\textbf{Navigation assistance for a semi-autonomous smart wheelchair}
\fontsize{11}{13.2}\selectfont

\vspace{11pt}

Jakob D. WYATT and S. Khaksar

\vspace{11pt}

Department of Mechanical Engineering, Curtin University, Perth, WA 6845, Australia

\vspace{11pt}

\end{center}
\begin{flushright}
\textit{siavash.khaksar@curtin.edu.au}
\end{flushright}

\begin{adjustwidth}{10mm}{10mm}
\section*{\textbf{ABSTRACT}}
% 10 lines
This progress report involves the identification and definition of the core thesis problem;
creating a semi-autonomous wheelchair that prioritises user safety and independence.
A literature review details prior work in this field, and identifies potential algorithms
for scene recognition and assistive control. Additionally, the literature review identifies and evaluates
sensors that could be used to perceive the wheelchairs environment.

A preliminary wheelchair driving video dataset has been collected around Curtin university.
This dataset has been used to evaluate algorithms such as YOLOv5, DeepLabv3, and Hybridnet on
the problem of scene recognition, with promising results. VFH+ has been evaluated as an assistive
control algorithm after being implemented in MATLAB. Additionally, sensors and hardware to be used
alongside the wheelchair have been identified and evaluated.
\end{adjustwidth}

\section*{\textbf{INTRODUCTION}}
The use of powered wheelchairs has enabled greater independence for people with disability,
however can be inaccessible or unsafe for people with visual impairment.
Smart wheelchairs add intelligent sensing and control to an existing powered wheelchair
in order to avoid obstacles in the environment. These wheelchairs can be fully-autonomous, moving the wheelchair
to an end goal, or semi-autonomous, where input from the user is blended with the control unit to improve safety.

Smart wheelchair implementations have used a variety of sensors to detect their environment, including RGB-D cameras (\cite{wangS2P2SelfSupervisedGoalDirected2021}),
2d Lidar, ultrasonic sensors (\cite{levineNavChairAssistiveWheelchair1999}), and mmWave radar.
Sensor fusion is often used to combine information from multiple sources to improve the model of the environment.
When evaluating a sensor for a smart wheelchair, factors such as cost, resolution, accuracy, and field of view are important.
The compute element inside a smart wheelchair often consists of a microcontroller to process user inputs and control the motors,
a general-purpose computer to run pathfinding algorithms and log information, and an AI accelerator (such as an Nvidia Jetson or Google Coral)
to improve the performance of machine learning algorithms.

These machine learning algorithms process input from sensors to understand the surrounding environment.
Image classification is a core problem within this field which involves identifying the subject of an image.
The use of convolutional neural networks (CNNs) and deep learning has vastly improved the accuracy of
classification models, with the 156 layer ResNet model (\cite{heDeepResidualLearning2016}) surpassing human performance
on the ImageNet dataset with an error rate of 3.57\%.
Object localisation involves classifying an object while also identifying its position within an image;
YOLO (\cite{redmonYouOnlyLook2015}) is a popular object localisation model which can run in real-time on many accelerators.
Semantic segmentation involves the classification of each pixel in an image, which is often used
on objects that cannot be cleanly identified with a bounding box.
The Hybridnets model \cite{vuHybridNetsEndtoEndPerception2022} is a model which performs drivable area segmentation.

Assistive control algorithms can be used to navigate the user through the environment once obstacles are detected.
Path-based algorithms such as A*
plan a path between a start and goal pose, aiming to reduce the distance travelled or acceleration experienced by the user.
In contrast, local algorithms such as VFH+ (\cite{ulrichVFHReliableObstacle1998})
set the wheelchair's current direction using a target direction, while avoiding nearby obstacles.

The overall aim of the project team, which consists of multiple project students and interns,
is to fully implement a smart semi-autonomous wheelchair
using an existing CentroGlide powered wheelchair.
The specific aim of this thesis is to implement navigation assistance,
which involves both avoiding environmental obstacles such as walls and stairs,
and guiding the user along pathways.
% Future work: Kerbs, Productionizing on chip
%\pagebreak % end of page

\section*{\textbf{RESULTS AND DISCUSSION}} % 2-3 figs/tables, 2/3-3/4 page
A ZED Mini RGB-D camera was mounted to the wheelchair to enable it to
sense the surrounding environment. To test the performance of the navigation
assistance system, a \SI{47}{\minute} wheelchair driving dataset was collected around
Curtin university.

The navigation assistance system involves identifying pathways and obstacles,
and placing these obstacles onto a 2D birds-eye view occupancy map of the
surrounding environment. Hybridnets was used to identify drivable
areas; the model was retrained on the Cityscapes dataset to improve its performance (\cref{table:retraining}).

\begin{table}[H]
    \centering
    \begin{tabular}{c c c}
    \toprule
    Metric (Cityscapes) & Pre-trained & After training \\
    \midrule
    mIoU & 26.6\% & 87.5\% \\
    Road Recall & 30.6\% & 96.2\% \\
    Sidewalk Recall & 1.5\% & 68.2\% \\
    \bottomrule
    \end{tabular}
    \caption{Comparison between dataset compression modes}
    \label{table:retraining}
\end{table}

To identify static obstacles such as walls, a custom algorithm was used to process 3D point cloud data.
An example of this algorithm can be seen in \cref{fig:algorithm},
with the wheelchair location identified with an `X' and obstacles shown in red.
This image also demonstrates drivable area segmentation, shown in green.
The algorithm correctly identifies the wall on the right and the handrail on the left as obstacles
and has a mean processing latency of \SI{550}{\milli\second} on a test laptop.
Due to the FOV of the camera, drivable areas and obstacles directly in front or to the side of
the wheelchair are not stored in the occupancy map.

\begin{figure}[H]
    % 16:6 width ratio
    % 0.64, 0.24
    \centering
    \begin{subfigure}{.35\textwidth}
        \centering
        \includegraphics[width=\linewidth]{images/segmentation.PNG}
    \end{subfigure}
    \quad
    \begin{subfigure}{.2\textwidth}
        \centering
        \includegraphics[width=\linewidth,frame]{images/map.jpg}
    \end{subfigure}
    \caption{Drivable area segmentation and obstacle detection (top-down occupancy map)}
    \label{fig:algorithm}
\end{figure}

A proof of concept semi-autonomous wheelchair control algorithm was implemented using VFH+.
This algorithm successfully modifies the wheelchair's direction to avoid obstacles using the
occupancy map. However, VFH+ does not change the wheelchair's speed.

\section*{\textbf{CONCLUSIONS}} % 5 lines
The results indicate that image segmentation is effective at identifying drivable areas outdoors.
The custom 3D point cloud processing algorithm works well at identifying walls,
however, is not yet fast enough for moving objects such as pedestrians.
The navigation assistance technology developed and tested in this thesis can be integrated
with other thesis students' work to create a semi-autonomous wheelchair. To work around the FOV
of the RGB-D camera, a SLAM algorithm could be used to keep obstacles in memory.

\section*{\textbf{ACKNOWLEDGEMENTS}}
The author wishes to thank Glide for providing a powered wheelchair to the research team.

\printbibliography[title=\textbf{REFERENCES},heading=bibliography] % don't need titles for journal articles, just page #

\end{document}
