\documentclass[10pt,twoside]{article}

\usepackage{setspace} \singlespacing % line spacing
\usepackage[top=2.5cm,bottom=2.5cm,inner=4cm,outer=2.5cm]{geometry} % margin sizes
\usepackage[none]{hyphenat} % dont hyphenate words
\usepackage{fontspec} \setmainfont{Times New Roman} % set font
\usepackage{microtype} % fix typing
\usepackage{amsmath} % equations
\usepackage{graphicx} % images
\usepackage{float} % figure positioning
\usepackage[hidelinks]{hyperref} % dont highlight links
\usepackage{cleveref} % easy to reference figures
\usepackage{siunitx} % format si units
\usepackage{changepage} % for the adjustwidth environment
\usepackage[authordate,backend=biber]{biblatex-chicago} \addbibresource{../zotero.bib} % referencing

\raggedbottom % allow pages to end early
\pdfvariable minorversion 7 % updated pdf version
\setlength\parindent{5mm} % change indent size
\pagenumbering{gobble} % no page numbers
\AtEveryBibitem{\clearfield{title}} % no article titles
\AtEveryBibitem{\clearfield{url}} % no url
\setlength\bibitemsep{0pt} % change separation between bib items
% Fix references before submission

% change section/subsection title font size to 10
\usepackage{sectsty}
\sectionfont{\fontsize{10}{10}\selectfont}
\subsectionfont{\fontsize{10}{10}\selectfont}
\subsubsectionfont{\fontsize{10}{10}\selectfont}

% change spacing before & after headings
\usepackage{titlesec}
\titlespacing*{\section}{0pt}{20pt}{6pt} % left, before, after
\titlespacing*{\subsection}{0pt}{20pt}{6pt}
\titlespacing*{\subsubsection}{0pt}{20pt}{6pt}

% change spacing around captions and equations
% indent equations
\setlength{\abovecaptionskip}{10pt}
\setlength{\abovedisplayskip}{10pt}
\setlength{\belowdisplayskip}{10pt}
\setlength{\abovedisplayshortskip}{10pt}
\setlength{\belowdisplayshortskip}{10pt}

\begin{document}

\begin{center}
\fontsize{12}{14.4}\selectfont
\textbf{Navigation assistance for a semi-autonomous smart wheelchair}
\fontsize{11}{13.2}\selectfont

\vspace{11pt}

Jakob D. WYATT and S. Khaksar

\vspace{11pt}

Department of Mechanical Engineering, Curtin University, Perth, WA 6845, Australia

\vspace{11pt}

\end{center}
\begin{flushright}
\textit{siavash.khaksar@curtin.edu.au}
\end{flushright}

\begin{adjustwidth}{10mm}{10mm}
\section*{\textbf{ABSTRACT}}
% 10 lines
This progress report involves the identification and definition of the core thesis problem;
creating a semi-autonomous wheelchair that prioritises user safety and independence.
A literature review details prior work in this field, and identifies potential algorithms
for scene recognition and assistive control. Additionally, the literature review identifies and evaluates
sensors that could be used to perceive the wheelchairs environment.

A preliminary wheelchair driving video dataset has been collected around Curtin university.
This dataset has been used to evaluate algorithms such as YOLOv5, DeepLabv3, and Hybridnet on
the problem of scene recognition, with promising results. VFH+ has been evaluated as an assistive
control algorithm after being implemented in MATLAB. Additionally, sensors and hardware to be used
alongside the wheelchair have been identified and evaluated.
\end{adjustwidth}

\section*{\textbf{INTRODUCTION}}
The use of powered wheelchairs has enabled greater independence for people with disability,
however can be inaccessible or unsafe for people with visual impairment.
Smart wheelchairs add intelligent sensing and control to an existing powered wheelchair
in order to avoid obstacles in the environment. These wheelchairs can be fully-autonomous, moving the wheelchair
to an end goal, or semi-autonomous, where input from the user is blended with the control unit to improve safety.

Smart wheelchair implementations have used a variety of sensors to detect their environment, including RGB-D cameras (\cite{wangS2P2SelfSupervisedGoalDirected2021}),
2d Lidar, ultrasonic sensors (\cite{levineNavChairAssistiveWheelchair1999}), and mmWave radar.
Sensor fusion is often used to combine information from multiple sources to improve the model of the environment.
When evaluating a sensor for a smart wheelchair, factors such as cost, resolution, accuracy, and field of view are important.
The compute element inside a smart wheelchair often consists of a microcontroller to process user inputs and control the motors,
a general-purpose computer to run pathfinding algorithms and log information, and an AI accelerator (such as an Nvidia Jetson or Google Coral)
to improve the performance of machine learning algorithms.

These machine learning algorithms process input from sensors to understand the surrounding environment.
Image classification is a core problem within this field which involves identifying the subject of an image.
The use of convolutional neural networks (CNNs) and deep learning has vastly improved the accuracy of
classification models, with the 156 layer ResNet model (\cite{heDeepResidualLearning2016}) surpassing human performance
on the ImageNet dataset with an error rate of 3.57\%.
Object localisation involves classifying an object while also identifying its position within an image;
YOLO (\cite{redmonYouOnlyLook2015}) is a popular object localisation model which can run in real-time on many accelerators.
Semantic segmentation involves the classification of each pixel in an image, using an algorithm such as DeepLab (\cite{chenRethinkingAtrousConvolution2017}).
This is often used for path detection or kerb detection, as these features cannot be cleanly identified with a bounding box.

Assistive control algorithms can be used to navigate the user through the environment once obstacles are detected.
Path-based algorithms such as A*
plan a path between a start and goal pose, aiming to reduce the distance travelled or acceleration experienced by the user.
In contrast, local algorithms such as VFH+ (\cite{ulrichVFHReliableObstacle1998})
set the wheelchair's current speed and direction using a target direction, while avoiding nearby obstacles.

The overall aim of the project team, which consists of multiple project students and interns,
is to fully implement a smart semi-autonomous wheelchair
using an existing CentroGlide powered wheelchair.
The specific aim of this thesis is to implement navigation assistance,
which involves both avoiding environmental obstacles such as walls and stairs,
and guiding the user along pathways.
% Future work: Kerbs, Productionizing on chip
%\pagebreak % end of page

\section*{\textbf{RESULTS AND DISCUSSION}} % 2-3 figs/tables, 2/3-3/4 page
Part of the current work this semester has included the selection of the sensor (Stereolabs Zed 2 Mini) and compute element (Nvidia Jetson),
as well as the identification of viable sensor mounting points on the wheelchair. This process has been detailed in the methodology.
A preliminary 34 minute driving dataset (1920x1080 @ 24 fps) has been collected around Curtin University using a GoPro Hero 4.
The experimental setup to collect this dataset can be seen here, including the identified
sensor mounting point.

\section*{\textbf{CONCLUSIONS}} % 5 lines
To evaluate this algorithm, a video encoder and decoder needed to be created. This was done using a Python generator,
so that frames in the video could simply be iterated over within a for loop and only decoded when required.
OpenCV \autocite{vuHybridNetsEndtoEndPerception2022} was the underlying library used to decode each video frame into a BGR pixel array.
After evaluation of the algorithm, each processed frame was displayed on the screen and encoded into a video file using OpenCV.

\section*{\textbf{ACKNOWLEDGEMENTS}}
The author wishes to thank Glide for providing a powered wheelchair to the research team.

\printbibliography[title=\textbf{REFERENCES},heading=bibliography] % don't need titles for journal articles, just page #

\end{document}
