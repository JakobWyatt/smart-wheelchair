\documentclass[12pt]{article}

% Specify document formatting.
\renewcommand{\baselinestretch}{1.5} 
\usepackage[top=2.5cm,bottom=2.5cm,inner=4cm,outer=2.5cm,twoside]{geometry}
\usepackage{fancyhdr} \usepackage{pdfpages} \usepackage[nottoc,notlof,notlot,numbib]{tocbibind}

% Change section title font size to 14.
\usepackage[none]{hyphenat} \usepackage{sectsty}
\sectionfont{\fontsize{14}{15}\selectfont}

\usepackage{fontspec}
\setmainfont{Times New Roman}

\usepackage{amsmath} \usepackage{graphicx} \usepackage{microtype} \usepackage{float}
\usepackage[hidelinks]{hyperref} \usepackage{cleveref}
\usepackage{siunitx} \usepackage{gensymb}
\usepackage{tabularx} \usepackage{booktabs} \usepackage{adjustbox}
\usepackage[style=ieee]{biblatex} \addbibresource{zotero.bib}

\begin{document}
\includepdf{title.pdf}
\thispagestyle{empty}
\pagebreak

\pagenumbering{roman}
\section*{Abstract}
%\addcontentsline{toc}{section}{Abstract}
\pagebreak

\renewcommand{\contentsname}{Table of Contents}
\tableofcontents
\listoffigures
\listoftables
\pagebreak

\pagenumbering{arabic}





\section{Introduction}
Many people with motor disabilities rely on wheelchairs for movement, and
powered wheelchairs have enabled greater independance for people with disability.
Despite the huge benefit powered wheelchairs have granted,
the use of this technology can be inaccessible or unsafe for people
with amyotrophic lateral sclerosis (ALS) or vision impairment,
who may be unable to use a joystick or see their environment clearly.


\subsection{Aims}
The aim of this research is to develop a semi-autonomous smart wheelchair system.
This research is done in collaboration with Glide, a WA wheelchair manufacturer,
who have provided an existing powered wheelchair (CentroGlide) to use as a base
for this functionality. By developing assistive technology for the wheelchair,
the user is granted greater mobility, confidence, and independence.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\linewidth]{images/wheelchair_reclined.jpeg}
    \caption{CentroGlide in Reclined Configuration}
    \label{fig:wheelchair_reclined}
\end{figure}

\pagebreak
\subsection{Problem Definition}
There are multiple engineering research project students who are part of this team,
working on elements such as controller design, navigation assistance, and object detection.
This work specifically focuses on pathway assistance, which identifies suitable
paths for the wheelchair to drive on. If a user unintentionally drives off their desired path,
this can lead to uneven terrain and possibly falling from the wheelchair.
By guiding the user along a path, these safety issues can be mitigated.

Emphasis is placed on the 'semi-autonomous' aspect of the wheelchair.
An important requirement of this project is that the user still
has control over their wheelchair, and can override any semi-autonomous functionality
if required. When false positives occur within the smart wheelchair system,
the users mobility should not be compromised.

Another requirement of the system is that any sensors mounted to the wheelchair
should not impede the users comfort or the wheelchairs manouverability.
Many wheelchair users have specific requirements for wheelchair seat adjustments,
to avoid pressure sores and discomfort. \Cref{fig:wheelchair_reclined} shows the
wheelchair configuration when fully reclined, demonstrating that some sensor mounting locations
are infeasible.

The smart wheelchair system should also be commercially viable - high-cost
components and sensors are infeasible. Internet connectivity should not be a requirement
for the system to operate either - the round trip time required to communicate with a server
would compromise the safety of a user. Because of this, all processing is performed locally
on the wheelchair.

\pagebreak





\section{Literature Review}
%Smart wheelchairs are wheelchairs with additional sensors and computers,
%enabling greater usability and safety. This can come in the form of alternative input methods,
%such as eye-gaze tracking \cite{eidNovelEyeGazeControlledWheelchair2016} or using a brain-computer
%interface \cite{kaufmannBraincomputerInterfaceBased2014} to control the wheelchair. For people with vision impairment,
%haptic feedback \cite{kondoNavigationGuidanceControl2008}\cite{vanderpoortenPoweredWheelchairNavigation2012}
%has been used to improve awareness of the surrounding environment and make indoor navigation safer.


\subsection{Sensors and Hardware}
%To percieve the environment, the wheelchair should be fitted with various sensors and
%a compute element to process the sensors output.

\subsubsection{Sensor Types}
Smart wheelchairs have used a varied range of sensor types to percieve the surrounding environment.
RGB-D stereo cameras have been widely used in the field \cite{wangS2P2SelfSupervisedGoalDirected2021}\cite{wangSelfSupervisedDrivableArea2019}\cite{jainAutomatedPerceptionSafe2014},
alongside 2d Lidar \cite{scudellariSelfdrivingWheelchairsDebut2017} and ultrasonic sensors \cite{levineNavChairAssistiveWheelchair1999}.
Self-driving cars built by companies such as Tesla and Waymo
use cameras, mmWave Radar, and 3d Lidar to avoid traffic and pedestrians.

Selecting a sensor to use is not necessarily an either-or decision. Sensor fusion algorithms such as
the Extended Kalman Filter (EKF) or Unscented Kalman Filter (UKF) \cite{wanUnscentedKalmanFilter2000} allow
outputs from multiple sensors to be used together to improve their accuracy. Additionally, sensors can
be used for different applications on the smart wheelchair - a stereo camera could be used to sense the surrounding environment
while an inertial measurement unit (IMU) could be used for wheelchair odometry.
\Cref{table:sensor_options} gives a comparison of several available sensor types,
taking into account factors such as resolution, cost, and accuracy.

\begin{table}[H]
    \centering
\begin{adjustbox}{width=\textwidth}
    \begin{tabular}{c c c}
    \toprule
    Sensor & Advantages & Disadvantages \\
    \midrule
    RGB-D Stereo Camera & Very high resolution & Low field of view (FOV) \\
    mmWave Radar & High accuracy & Low resolution \\
    3D Lidar & High resolution and accuracy & Very high cost \\
    2D Lidar & High FOV and accuracy & Only detects obstacles within the same plane \\
    Ultrasonic sensor & Low cost & One-dimensional \\
    \bottomrule
    \end{tabular}
\end{adjustbox}
    \caption{Sensor Comparisons}
    \label{table:sensor_options}
\end{table}

% Description of different sensor types?

\subsubsection{RGB-D Cameras}
One advantage RGB-D cameras have over alternative sensors is high RGB resolution,
allowing them to utilize recent advantages in machine learning and computer vision.
Sensors such as LIDAR may fail at path detection, as features such as drawn lines must
be distinguished visually from the surroundings.

When comparing these cameras, factors such as package size,
field of view, and depth accuracy are important to consider due to the available
mounting points on the wheelchair. Several commercial options are compared in \Cref{table:stereo_camera}
- all of the listed units come with an integrated IMU.

\begin{table}[H]
    \centering
\begin{adjustbox}{width=\textwidth}
    \begin{tabular}{c c c c c c}
    \toprule
    Name & Type & Cost (AUD)\footnote{Costs are taken at RRP with an exchange rate of 1 AUD = 0.74 USD} & Dimensions (mm) & FOV (Horizontal, Vertical, Depth) & Operating Range (m) \\
    \midrule
    Stereolabs Zed Mini \cite{stereolabsZEDMiniCamera2018} & Passive & \$595 & $124.5\times 30.5\times 26.5$ & $90\degree\times 60\degree\times 100\degree$ & 0.1-15\\
    Stereolabs Zed 2 \cite{stereolabsZEDCameraSDK2019} & Passive & \$670 & $175\times 30\times 33$ & $110\degree\times 70\degree\times 120\degree$ & 0.3-20 \\
    Intel RealSense D455 \cite{intelIntelRealSenseProduct2022} & Active IR (Stereo) & \$595 & $124\times 26\times 29$ & $90\degree\times 65\degree\times 87\degree$ & 0.6-6 \\
    Microsoft Azure Kinect DK \cite{microsoftAzureKinectDK2021} & Active IR (Time of Flight)\footnote{The Microsoft Azure Kinect DK has multiple operating modes which tradeoff between FOV, operating range, and resolution. The \texttt{NFOV unbinned} mode was compared, which provides good tradeoff between operating range and resolution.} & \$595 & $103\times 39\times 126$ & $75\degree\times 65\degree\times 75\degree$ & 0.5-3.86 \\
    \bottomrule
    \end{tabular}
\end{adjustbox}
    \caption{Stereo Camera Options}
    \label{table:stereo_camera}
\end{table}

A caveat of the Stereolabs products is that they require a seperate CUDA enabled GPU (Nvidia) to generate the point-cloud
and RGB-D image. The Kinect DK requires a CPU for processing, while the Intel RealSense performs processing onboard
and only requires a USB-C 3.1 interface to communicate.

\subsubection{Compute Element}
The compute element consists of several components - a microcontroller to process user inputs and
send signals to the motors, a general purpose computer to run pathfinding algorithms and log information,
and an AI accelerator to improve performance of on-board ML algorithms.

AI accelerators use specialized hardware to perform operations common in ML algorithms (such as matrix multiplication
and convolution) more efficiently than a CPU can. GPUs have been used widely for this application, however their high
power usage is infeasible for some applications. Embedded AI accelerators aim to provide greater power efficiency
at the cost of specialization.

The Nvidia Jetson and Google Coral products are both popular options for embedded AI acceleration. These accelerators are compared
in \Cref{table:compute_element}, alongside a GPU. Speed is difficult to compare across different products - originally, 

\begin{table}[H]
    \centering
\begin{adjustbox}{width=\textwidth}
    \begin{tabular}{c c c c c c}
    \toprule
    Name & Cost (AUD) & Speed & Power (W) & Notes \\
    \midrule
    Nvidia Jetson Nano \cite{stereolabsZEDMiniCamera2018} & \\
    Nvidia Jetson Xavier NX \cite{stereolabsZEDCameraSDK2019} & \\
    Nvidia GTX 1080 \cite{intelIntelRealSenseProduct2022} & \\
    Google Coral Edge TPU \cite{} & \\
    \bottomrule
    \end{tabular}
\end{adjustbox}
    \caption{AI Accelerator Options}
    \label{table:compute_element}
\end{table}

\pagebreak
\subsection{Scene Understanding}
Scene understanding is a broad field, and involves using computer vision methods
on visual or spatial data to gain better knowledge about the surrounding environment.
Convolutional Neural Networks (CNNs) are commonly used for this application, as they
are able to exploit the local nature of image features to reduce the number of required computations.

\subsubsection{Image Classification}
Image classification is a core problem within this field, and involves identifying the
subject of an image (such as an animal or object).
AlexNet \cite{krizhevskyImageNetClassificationDeep2012}, based on the earlier digit-recognition CNN LeNet-5
\cite{lecunGradientbasedLearningApplied1998}, was one of the first deep CNNs
applied to this problem. Alexnet was trained on the large ImageNet dataset \cite{jiadengImageNetLargescaleHierarchical2009},
which consists of 15M images and 22K categories,
and achieved an error of only 15.3\% on a 1000 class subset. The underlying architecture uses a series of 5 convolutional
layers and 3 fully connected layers.

Neural network architectures have become deeper and more accurate over time, enabled by both
growth in computational power and dataset size. VGG-16 \cite{simonyanVeryDeepConvolutional2014}
and GoogLeNet \cite{szegedyGoingDeeperConvolutions2014}
are 16 and 22 layers deep respectively, and approached
human performance on the ImageNet dataset. ResNet \cite{heDeepResidualLearning2016} is up to 156 layers deep,
and exceeds human performance at image classification with an error of 3.57\%.
ResNet uses a 'skipping' architecture to improve network training, where the output of a layer relies directly on
the input of a previous layer.

\subsubsection{Object Localization}
Object localization is another core problem within this field, and involves identifying the location of objects within an image,
as well as classifying them. This is important for our wheelchair, as we want to be able to identify the location of
a pedestrian or obstacle within the environment. R-CNN \cite{girshickRichFeatureHierarchies2013} was one of the
first object classification models which utilized convolutional networks, by identifying potential bounding boxes
and running an image classifier on these bounding boxes. Fast and Faster R-CNN \cite{girshickFastRCNN2015}\cite{renFasterRCNNRealTime2015}
improved the speed of this model by running an image classifier backbone once on the entire image, and using a CNN to improve
identification of bounding boxes. Pascal VOC \cite{everinghamPascalVisualObject2009} and MS COCO \cite{linMicrosoftCOCOCommon2014}
are datasets which are commonly used to evaluate object classification models.

YOLO (You Only Look Once) \cite{redmonYouOnlyLook2015}\cite{redmonYOLO9000BetterFaster2016}\cite{redmonYOLOv3IncrementalImprovement2018}\cite{bochkovskiyYOLOv4OptimalSpeed2020}
is another object classification model which focuses on improving performance. In particular, YOLOv4 \cite{bochkovskiyYOLOv4OptimalSpeed2020}
reaches over 60 fps on the Tesla V100, which enables its use in real time applications such as autonomous driving and security camera footage. % RCNN and YOLO
YOLO divides an image into an $S\times S$ grid, and uses a single convolutional network to output both bounding box predictions and
image classification for each grid square. Low-probability and overlapping bounding boxes are then removed before the final output.
%YOLOv4 uses a backbone-neck-head architecture

\subsubsection{Semantic Segmentation}

% U-net, deepnet

% classification, localization, segmentation, video, datasets
% SLAM, mapping, hybridnet, etc.


\subsection{Assistive Control}

% Inputs
% Semi-autonomy
% Full autonomy

% Indoor vs Outdoor assistance.
% Sensors
% Machine Learning
%\cite{tomariEnhancingWheelchairControl2014}
\pagebreak





\section{Methodology}

\subsection{Hardware}
The smart wheelchair should have the ability to sense, process, and manouver within the surrounding environment.
To do this requires some necessary hardware, including a sensor system, compute element, and motor controller.
Due to the 2021-2022 chip shortage, hardware selection was identified as a process that should occur relatively quickly.

The literature review provides a comparison between different sensor types and models. For outdoor navigation assistance,
a forward facing stereo camera was selected as the best option for this project - specifically, the Zed 2 Mini.
For the compute element, a Nvidia Jetson Xavier NX will be used, due to compatability with existing deep learning
frameworks and low power usage.

\subsection{Dataset Collection}
To train and evaluate machine learning models, a dataset was collected.

\pagebreak





\section{Current Work}

% 34 minute dataset
% algorithm:
% object detection, find out more about VSLAM, path planning
The first stages of smart wheelchair development involved:
\begin{enumerate}
    \item Identifying desired sensors and hardware for the wheelchair.
    \item Choosing an appropriate mounting point for these sensors.
    \item Researching the field of machine perception and computer vision (both applied to wheelchairs and more generally).
    \item Collecting an initial video dataset, enabling work to begin on labelling and algorithm evaluation.
\end{enumerate}

\subsection{Hardware}

After consideration of the available options, it was decided to use a stereo camera as the main
forward facing sensor, with 2D LIDAR used for the side and rear of the wheelchair.

The front of the joystick control unit was selected as the best mounting point for the
stereo camera, due to several reasons:
\begin{enumerate}
    \item Clear view of the environment in front of the wheelchair.
    \item Not obstructed by the user in any wheelchair configuration.
    \item When needed, the user can move the joystick control unit out of the way,
            which also moves the camera out of the way.
\end{enumerate}
However, there are some challenges faced when using this mounting point, which must be addressed.
\begin{enumerate}
    \item Shaky video footage due to low rigidity in joystick mount.
    \item Close to the front of the wheelchair, which reduces visibility of the sides of the wheelchair.
    \item Maximum camera width of \SI{150}{\milli\metre} before doorway manouverability is affected.
\end{enumerate}

To evaluate the effectiveness of this mounting point, a GoPro (Hero 4) was attached
using a temporary mount and a 34 minute driving dataset was collected around Curtin University.
% Images here
It was found that camera shakiness could be reduced by using a stiffer mounting solution,
however some shakiness would always remain due to the unstable mounting surface.
It was also found that alternative sensors, such as 2D LIDAR, would be required for features such as doorway
navigation and docking, due to the low field of view (FOV).

\pagebreak





\section{Future Work}

\pagebreak





\printbibliography[heading=bibnumbered]

\end{document}
