
@article{baileySimultaneousLocalizationMapping2006,
  title = {Simultaneous {{Localization}} and {{Mapping}} ({{SLAM}}): {{Part II}}},
  author = {Bailey, T and Durrant-Whyte, H},
  date = {2006},
  journaltitle = {IEEE Robotics \& Automation Magazine},
  shortjournal = {MRA},
  volume = {13},
  number = {3},
  pages = {108--117},
  publisher = {{PISCATAWAY: IEEE}},
  location = {{PISCATAWAY}},
  issn = {1070-9932},
  doi = {doi.org/10.1109/MRA.2006.1678144},
  abstract = {This paper discusses the recursive Bayesian formulation of the simultaneous localization and mapping (SLAM) problem in which probability distributions or estimates of absolute or relative locations of landmarks and vehicle pose are obtained. The paper focuses on three key areas: computational complexity},
  keywords = {Recommended,Survey},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/SLAM PART II.pdf}
}

@article{bochkovskiyYOLOv4OptimalSpeed2020,
  title = {{{YOLOv4}}: {{Optimal Speed}} and {{Accuracy}} of {{Object Detection}}},
  author = {Bochkovskiy, Alexey and Wang, Chien-Yao and Liao, Hong-Yuan Mark},
  date = {2020},
  doi = {doi.org/10.48550/arXiv.2004.10934},
  abstract = {There are a huge number of features which are said to improve Convolutional Neural Network (CNN) accuracy. Practical testing of combinations of such features on large datasets, and theoretical justification of the result, is required. Some features operate on certain models exclusively and for certain problems exclusively, or only for small-scale datasets},
  keywords = {Machine Learning,Model},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/bochkovskiyYOLOv4OptimalSpeed2020.pdf}
}

@inproceedings{dawoodDistanceMeasurementSelfDriving2017,
  title = {Distance {{Measurement}} for {{Self-Driving Cars Using Stereo Camera}}},
  booktitle = {Proceedings of the 6th {{International Conference}} of {{Computing}} \& {{Informatics}}},
  author = {Dawood, Yasir and Ku-Mahamud, Ku and Kamioka, Eiji},
  date = {2017},
  pages = {235--242},
  abstract = {Self-driving cars reduce human error and can accomplish various missions to help people in different fields. They have become one of the main interests in automotive research and development, both in the industry and academia. However, many challenges are encountered in dealing with distance measurement and cost, both in equipment and technique. The use of stereo camera to measure the distance of an object is convenient and popular for obstacle avoidance and navigation of autonomous vehicles. The calculation of distance considers angular distance, distance between cameras, and the pixel of the image. This study proposes a method that measures object distance based on trigonometry, that is, facing the self-driving car using image processing and stereo vision with high accuracy, low cost, and computational speed. The method achieves a high distance measuring accuracy of up to 20 m. It can be implemented in real time computing systems and can determine the safe driving distance between obstacles.},
  keywords = {Recommended},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/Distance measurement.pdf}
}

@article{durrant-whyteSimultaneousLocalizationMapping2006,
  title = {Simultaneous {{Localization}} and {{Mapping}}: {{Part I}}},
  author = {Durrant-Whyte, H and Bailey, T},
  date = {2006},
  journaltitle = {IEEE Robotics \& Automation Magazine},
  shortjournal = {MRA},
  volume = {13},
  number = {2},
  pages = {99--110},
  publisher = {{PISCATAWAY: IEEE}},
  location = {{PISCATAWAY}},
  issn = {1070-9932},
  doi = {doi.org/10.1109/MRA.2006.1638022},
  abstract = {This paper describes the simultaneous localization and mapping (SLAM) problem and the essential methods for solving the SLAM problem and summarizes key implementations and demonstrations of the method. While there are still many practical issues to overcome, especially in more complex outdoor environments, the general SLAM method is now a well understood and established part of robotics. Another part of the tutorial summarized more recent works in addressing some of the remaining issues in SLAM, including computation, feature representation, and data association.},
  keywords = {Recommended,Survey},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/SLAM PART I.pdf}
}

@article{eidNovelEyeGazeControlledWheelchair2016,
  title = {A {{Novel Eye-Gaze-Controlled Wheelchair System}} for {{Navigating Unknown Environments}}: {{Case Study With}} a {{Person With ALS}}},
  author = {Eid, Mohamad A and Giakoumidis, Nikolas and El Saddik, Abdulmotaleb},
  date = {2016},
  journaltitle = {IEEE access},
  shortjournal = {Access},
  volume = {4},
  pages = {558--573},
  publisher = {{PISCATAWAY: IEEE}},
  location = {{PISCATAWAY}},
  issn = {2169-3536},
  doi = {doi.org/10.1109/ACCESS.2016.2520093},
  abstract = {Thanks to advances in electric wheelchair design, persons with motor impairments due to diseases, such as amyotrophic lateral sclerosis (ALS), have tools to become more independent and mobile. However, an electric wheelchair generally requires considerable skill to learn how to use and operate. Moreover, some persons with motor disabilities cannot drive an electric wheelchair manually (even with a joystick), because they lack the physical ability to control their hand movement (such is the case with people with ALS). In this paper, we propose a novel system that enables a person with motor disability to control a wheelchair via eye-gaze and to provide a continuous, real-time navigation in unknown environments. The system comprises a Permobile M400 wheelchair, eye tracking glasses, a depth camera to capture the geometry of the ambient space, a set of ultrasound and infrared sensors to detect obstacles with low proximity that are out of the field of view for the depth camera, a laptop placed on a flexible mount for maximized comfort, and a safety off switch to turn off the system whenever needed. First, a novel algorithm is proposed to support continuous, real-time target identification, path planning, and navigation in unknown environments. Second, the system utilizes a novel N-cell grid-based graphical user interface that adapts to input/output interfaces specifications. Third, a calibration method for the eye tracking system is implemented to minimize the calibration overheads. A case study with a person with ALS is presented, and interesting findings are discussed. The participant showed improved performance in terms of calibration time, task completion time, and navigation speed for a navigation trips between office, dining room, and bedroom. Furthermore, debriefing the caregiver has also shown promising results: the participant enjoyed higher level of confidence driving the wheelchair and experienced no collisions through all the experiment.},
  keywords = {Wheelchairs},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/eidNovelEyeGazeControlledWheelchair2016.pdf}
}

@inproceedings{goilUsingMachineLearning2013,
  title = {Using Machine Learning to Blend Human and Robot Controls for Assisted Wheelchair Navigation},
  booktitle = {2013 {{IEEE}} 13th {{International Conference}} on {{Rehabilitation Robotics}} ({{ICORR}})},
  author = {Goil, Aditya and Derry, Matthew and Argall, Brenna D},
  date = {2013},
  volume = {2013},
  pages = {1--6},
  publisher = {{United States: IEEE}},
  location = {{United States}},
  doi = {doi.org/10.1109/ICORR.2013.6650454},
  abstract = {This work presents an algorithm for collaborative control of an assistive semi-autonomous wheelchair. Our approach is based on a statistical machine learning technique to learn task variability from demonstration examples. The algorithm has been developed in the context of shared-control powered wheelchairs that provide assistance to individuals with impairments that affect their control in challenging driving scenarios, like doorway navigation. We validate our algorithm within a simulation environment, and find that with relatively few demonstrations, our approach allows for safe traversal of the doorway while maintaining a high level of user control.},
  eventtitle = {2013 {{IEEE}} 13th {{International Conference}} on {{Rehabilitation Robotics}} ({{ICORR}})},
  isbn = {1945-7901},
  keywords = {Wheelchairs},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/goilUsingMachineLearning2013.pdf}
}

@inproceedings{jainAutomatedPerceptionSafe2014,
  title = {Automated Perception of Safe Docking Locations with Alignment Information for Assistive Wheelchairs},
  booktitle = {{{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}}},
  author = {Jain, Siddarth and Argall, Brenna},
  date = {2014},
  pages = {4997--5002},
  publisher = {{IEEE}},
  doi = {doi.org/10.1109/IROS.2014.6943272},
  abstract = {There are basic manuvering tasks with a powered wheelchair, like docking under a table and passage through a doorway or narrow hallway, which can be difficult for users with severe motor impairments - not only because of limitations in their own motor control, but also because of limitations in the control interfaces available to them. Robot automation can help transfer some of this control burden from the user to the machine. This work presents an algorithm for the automated detection of safe docking locations at rectangular and circular docking structures (tables, desks) with proper alignment information using 3D point cloud data. The safe docking locations can then be provided as goals to an autonomous path planner, within the context of providing adaptive driving assistance for powered wheelchair users. We evaluate the performance of our algorithm with systematic testing on several docking structures, observed from varied viewpoints.},
  isbn = {2153-0858},
  keywords = {From Survey,Wheelchairs},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/jainAutomatedPerceptionSafe2014.pdf}
}

@video{karpathyTeslaAIDay2021,
  title = {Tesla {{AI Day}}},
  editor = {Karpathy, Andrej},
  date = {2021},
  url = {youtube.com/watch?v=j0z4FweCy4M},
  editortype = {director}
}

@article{leamanComprehensiveReviewSmart2017,
  title = {A {{Comprehensive Review}} of {{Smart Wheelchairs}}: {{Past}}, {{Present}}, and {{Future}}},
  author = {Leaman, Jesse and {Hung Manh La}},
  date = {2017},
  journaltitle = {IEEE Transactions on Human-Machine Systems},
  shortjournal = {THMS},
  volume = {47},
  number = {4},
  pages = {486--499},
  publisher = {{PISCATAWAY: IEEE}},
  location = {{PISCATAWAY}},
  issn = {2168-2291},
  doi = {doi.org/10.1109/THMS.2017.2706727},
  abstract = {A smart wheelchair (SW) is a power wheelchair (PW) to which computers, sensors, and assistive technology are attached. In the past decade, there has been little effort to provide a systematic review of SW research. This paper aims to provide a complete state-of-the-art overview of SW research trends. We expect that the information gathered in this study will enhance awareness of the status of contemporary PW as well as SW technology and increase the functional mobility of people who use PWs. We systematically present the international SW research effort, starting with an introduction to PWs and the communities they serve. Then, we discuss in detail the SW and associated technological innovations with an emphasis on the most researched areas, generating the most interest for future research and development. We conclude with our vision for the future of SW research and how to best serve people with all types of disabilities.},
  keywords = {Survey,Wheelchairs},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/leamanComprehensiveReviewSmart2017.pdf}
}

@article{linMicrosoftCOCOCommon2014,
  title = {Microsoft {{COCO}}: {{Common Objects}} in {{Context}}},
  author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Bourdev, Lubomir and Girshick, Ross and Hays, James and Perona, Pietro and Ramanan, Deva and Zitnick, C. Lawrence and Dollár, Piotr},
  date = {2014},
  doi = {doi.org/10.48550/arXiv.1405.0312},
  abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
  keywords = {Dataset,Machine Learning},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/linMicrosoftCOCOCommon2014.pdf}
}

@article{lugaresiMediaPipeFrameworkBuilding2019,
  title = {{{MediaPipe}}: {{A Framework}} for {{Building Perception Pipelines}}},
  author = {Lugaresi, Camillo and Tang, Jiuqiang and Nash, Hadon and McClanahan, Chris and Uboweja, Esha and Hays, Michael and Zhang, Fan and Chang, Chuo-Ling and Yong, Ming Guang and Lee, Juhyun and Chang, Wan-Teh and Hua, Wei and Georg, Manfred and Grundmann, Matthias},
  date = {2019},
  doi = {doi.org/10.48550/arXiv.1906.08172},
  abstract = {Building applications that perceive the world around them is challenging. A developer needs to (a) select and develop corresponding machine learning algorithms and models, (b) build a series of prototypes and demos, (c) balance resource consumption against the quality of the solutions, and finally (d) identify and mitigate problematic cases. The MediaPipe framework addresses all of these challenges. A developer can use MediaPipe to build prototypes by combining existing perception components, to advance them to polished cross-platform applications and measure system performance and resource consumption on target platforms. We show that these features enable a developer to focus on the algorithm or model development and use MediaPipe as an environment for iteratively improving their application with results reproducible across different devices and platforms. MediaPipe will be open-sourced at https://github.com/google/mediapipe.},
  keywords = {Framework,Machine Learning},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/lugaresiMediaPipeFrameworkBuilding2019.pdf}
}

@article{redmonYOLO9000BetterFaster2016,
  title = {{{YOLO9000}}: {{Better}}, {{Faster}}, {{Stronger}}},
  author = {Redmon, Joseph and Farhadi, Ali},
  date = {2016},
  doi = {doi.org/10.48550/arXiv.1612.08242},
  abstract = {We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster RCNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on object detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that don't have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. But YOLO can detect more than just 200 classes},
  keywords = {Machine Learning,Model},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/redmonYOLO9000BetterFaster2016.pdf}
}

@article{redmonYOLOv3IncrementalImprovement2018,
  title = {{{YOLOv3}}: {{An Incremental Improvement}}},
  author = {Redmon, Joseph and Farhadi, Ali},
  date = {2018},
  doi = {doi.org/10.48550/arXiv.1804.02767},
  abstract = {We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network that's pretty swell. It's a little bigger than last time but more accurate. It's still fast though, don't worry. At 320x320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. When we look at the old .5 IOU mAP detection metric YOLOv3 is quite good. It achieves 57.9 mAP@50 in 51 ms on a Titan X, compared to 57.5 mAP@50 in 198 ms by RetinaNet, similar performance but 3.8x faster. As always, all the code is online at https://pjreddie.com/yolo/},
  keywords = {Machine Learning,Model},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/redmonYOLOv3IncrementalImprovement2018.pdf}
}

@article{redmonYouOnlyLook2015,
  title = {You {{Only Look Once}}: {{Unified}}, {{Real-Time Object Detection}}},
  author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  date = {2015},
  doi = {doi.org/10.48550/arXiv.1506.02640},
  abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
  keywords = {Machine Learning,Model},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/redmonYouOnlyLook2015.pdf}
}

@article{ryuDevelopmentAutonomousDriving2022,
  title = {Development of an {{Autonomous Driving Smart Wheelchair}} for the {{Physically Weak}}},
  author = {Ryu, Hye-Yeon and Kwon, Je-Seong and Lim, Jeong-Hak and Kim, A-Hyeon and Baek, Su-Jin and Kim, Jong-Wook},
  date = {2022},
  journaltitle = {Applied sciences},
  volume = {12},
  number = {1},
  pages = {377},
  publisher = {{MDPI AG}},
  issn = {2076-3417},
  doi = {doi.org/10.3390/app12010377},
  abstract = {People who have difficulty moving owing to problems in walking spend their lives assisted by wheelchairs. In the past, research has been conducted regarding the application of various technologies to electric wheelchairs for user convenience. In this study, we evaluated a method of applying an autonomous driving function and developed an autonomous driving function using ROS. An electric wheelchair with a control unit designed to enable autonomous driving was used to test the basic performance of autonomous driving. The effectiveness of the technology was confirmed by comparing the results of autonomous driving with those of manual driving on the same route. It is expected that the evaluation and improvement of the usability and ride quality as well as additional studies will help improve the mobility convenience of physically disabled persons.},
  keywords = {Wheelchairs},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/ryuDevelopmentAutonomousDriving2022.pdf}
}

@article{scudellariSelfdrivingWheelchairsDebut2017,
  title = {Self-Driving Wheelchairs Debut in Hospitals and Airports [{{News}}]},
  author = {Scudellari, Megan},
  date = {2017},
  journaltitle = {IEEE Spectrum},
  shortjournal = {SPEC},
  volume = {54},
  number = {10},
  pages = {14--14},
  publisher = {{PISCATAWAY: IEEE}},
  location = {{PISCATAWAY}},
  issn = {0018-9235},
  doi = {doi.org/10.1109/MSPEC.2017.8048827},
  abstract = {Autonomous vehicles can add a new member to their ranks-the self-driving wheelchair. This summer, two robotic wheelchairs made headlines: one at a Singaporean hospital and another at a Japanese airport. The Singapore-MIT Alliance for Research and Technology, or SMART, developed the former, first deployed in Singapore's Changi General Hospital in September 2016, where it successfully navigated the hospital's hallways. It is the latest in a string of autonomous vehicles made by SMART, including a golf cart, an electric taxi, and most recently, a scooter that zipped more than 100 MIT visitors around on tours in 2016.},
  keywords = {News,Wheelchairs},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/scudellariSelfdrivingWheelchairsDebut2017.pdf}
}

@article{taketomiVisualSLAMAlgorithms2017,
  title = {Visual {{SLAM}} Algorithms: A Survey from 2010 to 2016},
  author = {Taketomi, Takafumi and Uchiyama, Hideaki and Ikeda, Sei},
  date = {2017},
  journaltitle = {IPSJ Transactions on Computer Vision and Applications},
  shortjournal = {IPSJ T Comput Vis Appl},
  volume = {9},
  number = {1},
  pages = {1--11},
  publisher = {{Berlin/Heidelberg: Springer Berlin Heidelberg}},
  location = {{Berlin/Heidelberg}},
  issn = {1882-6695},
  doi = {doi.org/10.1186/s41074-017-0027-2},
  abstract = {SLAM is an abbreviation for simultaneous localization and mapping, which is a technique for estimating sensor motion and reconstructing structure in an unknown environment. Especially, Simultaneous Localization and Mapping (SLAM) using cameras is referred to as visual SLAM (vSLAM) because it is based on visual information only. vSLAM can be used as a fundamental technology for various types of applications and has been discussed in the field of computer vision, augmented reality, and robotics in the literature. This paper aims to categorize and summarize recent vSLAM algorithms proposed in different research communities from both technical and historical points of views. Especially, we focus on vSLAM algorithms proposed mainly from 2010 to 2016 because major advance occurred in that period. The technical categories are summarized as follows: feature-based, direct, and RGB-D camera-based approaches.},
  keywords = {Recommended,Survey},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/VSLAM Survey.pdf}
}

@incollection{thrunRoboticMappingSurvey2002,
  title = {Robotic {{Mapping}}: {{A Survey}}},
  booktitle = {Exploring {{Artificial Intelligence}} in the {{New Millenium}}},
  author = {Thrun, Sebastian},
  date = {2002},
  publisher = {{Morgan Kaufmann}},
  url = {robots.stanford.edu/papers/thrun.mapping-tr.html},
  abstract = {This article provides a comprehensive introduction into the field of robotic mapping, with a focus on indoor mapping. It describes and compares various probabilistic techniques, as they are presently being applied to a vast array of mobile robot mapping problems. The history of robotic mapping is also described, along with an extensive list of open research problems.},
  keywords = {Recommended,Survey},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/EFK Survey.pdf}
}

@inproceedings{tomariEnhancingWheelchairControl2014,
  title = {Enhancing {{Wheelchair}}’s {{Control Operation}} of a {{Severe Impairment User}}},
  booktitle = {The 8th {{International Conference}} on {{Robotic}}, {{Vision}}, {{Signal Processing}} \& {{Power Applications}}},
  author = {Tomari, Mohd Razali Md and Kobayashi, Yoshinori and Kuno, Yoshinori},
  editor = {Mat Sakim, Harsa Amylia and Mustaffa, Mohd Tafir},
  date = {2014},
  pages = {65--72},
  publisher = {{Springer Singapore}},
  location = {{Singapore}},
  doi = {10.1007/978-981-4585-42-2},
  abstract = {Users with severe motor ability are unable to control their wheelchair using standard joystick and hence an alternative control input is preferred. However, using such an input, undoubtedly the navigation burden for the user is significantly increased. In this paper a method on how to reduce such a burden with the help of smart navigation platform is proposed. Initially, user information is inferred using an IMU sensor and a bite-like switch. Then information from the environment is obtained using combination of laser and Kinect sensors. Eventually, both information from the environment and the user is analyzed to decide the final control operation that according to the user intention, safe and comfortable to the people in the surrounding. Experimental results demonstrate the feasibility of the proposed approach.},
  isbn = {978-981-4585-42-2},
  keywords = {From Survey,Wheelchairs},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/tomariEnhancingWheelchairControl2014.pdf}
}

@inproceedings{vanderpoortenPoweredWheelchairNavigation2012,
  title = {Powered Wheelchair Navigation Assistance through Kinematically Correct Environmental Haptic Feedback},
  booktitle = {{{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  author = {Vander Poorten, E. B and Demeester, E and Reekmans, E and Philips, J and Huntemann, A and De Schutter, J},
  date = {2012},
  pages = {3706--3712},
  publisher = {{IEEE}},
  doi = {doi.org/10.1109/ICRA.2012.6225349},
  abstract = {This article introduces a set of novel haptic guidance algorithms intended to provide intuitive and reliable assistance for electric wheelchair navigation through narrow or crowded spaces. The proposed schemes take hereto the non-holonomic nature and a detailed geometry of the wheelchair into consideration. The methods encode the environment as a set of collision-free circular paths and, making use of a model-free impedance controller, `haptically' guide the user along collision-free paths or away from obstructed paths or paths that simply do not coincide with the motion intended by the user. The haptic feedback plays a central role as it establishes a fast bilateral communication channel between user and wheelchair controller and allows a direct negotiation about wheelchair motion. If found unsatisfactory, suggested trajectories can always be overruled by the user. Relying on inputs from user modeling and intention recognition schemes, the system can reduce forces needed to move along intended directions, thereby avoiding unnecessary fatigue of the user. A commercial powered wheelchair was upgraded and feasability tests were conducted to validate the proposed methods. The potential of the proposed approaches was hereby demonstrated.},
  isbn = {1050-4729},
  keywords = {From Survey,Wheelchairs},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/vanderpoortenPoweredWheelchairNavigation2012.pdf}
}

@article{vaswaniAttentionAllYou2017,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
  date = {2017},
  doi = {doi.org/10.48550/arXiv.1706.03762},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/vaswaniAttentionAllYou2017.pdf}
}

@article{wangSelfSupervisedDrivableArea2019,
  title = {Self-{{Supervised Drivable Area}} and {{Road Anomaly Segmentation Using RGB-D Data For Robotic Wheelchairs}}},
  author = {Wang, Hengli and Sun, Yuxiang and Liu, Ming},
  date = {2019},
  journaltitle = {IEEE Robotics and Automation Letters},
  shortjournal = {LRA},
  volume = {4},
  number = {4},
  pages = {4386--4393},
  publisher = {{IEEE}},
  issn = {2377-3766},
  doi = {doi.org/10.1109/LRA.2019.2932874},
  abstract = {The segmentation of drivable areas and road anomalies are critical capabilities to achieve autonomous navigation for robotic wheelchairs. The recent progress of semantic segmentation using deep learning techniques has presented effective results. However, the acquisition of large-scale datasets with hand-labeled ground truth is time-consuming and labor-intensive, making the deep learning-based methods often hard to implement in practice. We contribute to the solution of this problem for the task of drivable area and road anomaly segmentation by proposing a self-supervised learning approach. We develop a pipeline that can automatically generate segmentation labels for drivable areas and road anomalies. Then, we train RGB-D data-based semantic segmentation neural networks and get predicted labels. Experimental results show that our proposed automatic labeling pipeline achieves an impressive speed-up compared to manual labeling. In addition, our proposed self-supervised approach exhibits more robust and accurate results than the state-of-the-art traditional algorithms as well as the state-of-the-art self-supervised algorithms.},
  keywords = {Wheelchairs},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/wangSelfSupervisedDrivableArea2019.pdf}
}


