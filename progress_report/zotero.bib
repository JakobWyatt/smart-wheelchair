
@article{baileySimultaneousLocalizationMapping2006,
  title = {Simultaneous {{Localization}} and {{Mapping}} ({{SLAM}}): {{Part II}}},
  author = {Bailey, T and Durrant-Whyte, H},
  date = {2006},
  journaltitle = {IEEE Robotics \& Automation Magazine},
  shortjournal = {MRA},
  volume = {13},
  number = {3},
  pages = {108--117},
  publisher = {{PISCATAWAY: IEEE}},
  location = {{PISCATAWAY}},
  issn = {1070-9932},
  doi = {doi.org/10.1109/MRA.2006.1678144},
  abstract = {This paper discusses the recursive Bayesian formulation of the simultaneous localization and mapping (SLAM) problem in which probability distributions or estimates of absolute or relative locations of landmarks and vehicle pose are obtained. The paper focuses on three key areas: computational complexity},
  keywords = {Recommended,Survey},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/SLAM PART II.pdf}
}

@article{bochkovskiyYOLOv4OptimalSpeed2020,
  title = {{{YOLOv4}}: {{Optimal Speed}} and {{Accuracy}} of {{Object Detection}}},
  author = {Bochkovskiy, Alexey and Wang, Chien-Yao and Liao, Hong-Yuan Mark},
  date = {2020},
  doi = {doi.org/10.48550/arXiv.2004.10934},
  abstract = {There are a huge number of features which are said to improve Convolutional Neural Network (CNN) accuracy. Practical testing of combinations of such features on large datasets, and theoretical justification of the result, is required. Some features operate on certain models exclusively and for certain problems exclusively, or only for small-scale datasets},
  keywords = {Machine Learning,Model},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/bochkovskiyYOLOv4OptimalSpeed2020.pdf}
}

@software{bradskiOpenCVLibrary2000,
  title = {The {{OpenCV Library}}},
  author = {Bradski, Gary},
  date = {2000},
  url = {https://opencv.org/},
  keywords = {Framework}
}

@inproceedings{dawoodDistanceMeasurementSelfDriving2017,
  title = {Distance {{Measurement}} for {{Self-Driving Cars Using Stereo Camera}}},
  booktitle = {Proceedings of the 6th {{International Conference}} of {{Computing}} \& {{Informatics}}},
  author = {Dawood, Yasir and Ku-Mahamud, Ku and Kamioka, Eiji},
  date = {2017},
  pages = {235--242},
  abstract = {Self-driving cars reduce human error and can accomplish various missions to help people in different fields. They have become one of the main interests in automotive research and development, both in the industry and academia. However, many challenges are encountered in dealing with distance measurement and cost, both in equipment and technique. The use of stereo camera to measure the distance of an object is convenient and popular for obstacle avoidance and navigation of autonomous vehicles. The calculation of distance considers angular distance, distance between cameras, and the pixel of the image. This study proposes a method that measures object distance based on trigonometry, that is, facing the self-driving car using image processing and stereo vision with high accuracy, low cost, and computational speed. The method achieves a high distance measuring accuracy of up to 20 m. It can be implemented in real time computing systems and can determine the safe driving distance between obstacles.},
  keywords = {Recommended},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/Distance measurement.pdf}
}

@article{durrant-whyteSimultaneousLocalizationMapping2006,
  title = {Simultaneous {{Localization}} and {{Mapping}}: {{Part I}}},
  author = {Durrant-Whyte, H and Bailey, T},
  date = {2006},
  journaltitle = {IEEE Robotics \& Automation Magazine},
  shortjournal = {MRA},
  volume = {13},
  number = {2},
  pages = {99--110},
  publisher = {{PISCATAWAY: IEEE}},
  location = {{PISCATAWAY}},
  issn = {1070-9932},
  doi = {doi.org/10.1109/MRA.2006.1638022},
  abstract = {This paper describes the simultaneous localization and mapping (SLAM) problem and the essential methods for solving the SLAM problem and summarizes key implementations and demonstrations of the method. While there are still many practical issues to overcome, especially in more complex outdoor environments, the general SLAM method is now a well understood and established part of robotics. Another part of the tutorial summarized more recent works in addressing some of the remaining issues in SLAM, including computation, feature representation, and data association.},
  keywords = {Recommended,Survey},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/SLAM PART I.pdf}
}

@article{eidNovelEyeGazeControlledWheelchair2016,
  title = {A {{Novel Eye-Gaze-Controlled Wheelchair System}} for {{Navigating Unknown Environments}}: {{Case Study With}} a {{Person With ALS}}},
  author = {Eid, Mohamad A and Giakoumidis, Nikolas and El Saddik, Abdulmotaleb},
  date = {2016},
  journaltitle = {IEEE access},
  shortjournal = {Access},
  volume = {4},
  pages = {558--573},
  publisher = {{PISCATAWAY: IEEE}},
  location = {{PISCATAWAY}},
  issn = {2169-3536},
  doi = {doi.org/10.1109/ACCESS.2016.2520093},
  abstract = {Thanks to advances in electric wheelchair design, persons with motor impairments due to diseases, such as amyotrophic lateral sclerosis (ALS), have tools to become more independent and mobile. However, an electric wheelchair generally requires considerable skill to learn how to use and operate. Moreover, some persons with motor disabilities cannot drive an electric wheelchair manually (even with a joystick), because they lack the physical ability to control their hand movement (such is the case with people with ALS). In this paper, we propose a novel system that enables a person with motor disability to control a wheelchair via eye-gaze and to provide a continuous, real-time navigation in unknown environments. The system comprises a Permobile M400 wheelchair, eye tracking glasses, a depth camera to capture the geometry of the ambient space, a set of ultrasound and infrared sensors to detect obstacles with low proximity that are out of the field of view for the depth camera, a laptop placed on a flexible mount for maximized comfort, and a safety off switch to turn off the system whenever needed. First, a novel algorithm is proposed to support continuous, real-time target identification, path planning, and navigation in unknown environments. Second, the system utilizes a novel N-cell grid-based graphical user interface that adapts to input/output interfaces specifications. Third, a calibration method for the eye tracking system is implemented to minimize the calibration overheads. A case study with a person with ALS is presented, and interesting findings are discussed. The participant showed improved performance in terms of calibration time, task completion time, and navigation speed for a navigation trips between office, dining room, and bedroom. Furthermore, debriefing the caregiver has also shown promising results: the participant enjoyed higher level of confidence driving the wheelchair and experienced no collisions through all the experiment.},
  keywords = {Wheelchairs},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/eidNovelEyeGazeControlledWheelchair2016.pdf}
}

@article{everinghamPascalVisualObject2009,
  title = {The {{Pascal Visual Object Classes}} ({{VOC}}) {{Challenge}}},
  author = {Everingham, Mark and Van Gool, Luc and Williams, Christopher K. I and Winn, John and Zisserman, Andrew},
  date = {2009},
  journaltitle = {International Journal of Computer Vision},
  shortjournal = {Int J Comput Vis},
  volume = {88},
  number = {2},
  pages = {303--338},
  publisher = {{Boston: Springer US}},
  location = {{Boston}},
  issn = {0920-5691},
  doi = {10.1007/s11263-009-0275-4},
  abstract = {The Pascal Visual Object Classes (VOC) challenge is a benchmark in visual object category recognition and detection, providing the vision and machine learning communities with a standard dataset of images and annotation, and standard evaluation procedures. Organised annually from 2005 to present, the challenge and its associated dataset has become accepted as the benchmark for object detection. This paper describes the dataset and evaluation procedure. We review the state-of-the-art in evaluated methods for both classification and detection, analyse whether the methods are statistically different, what they are learning from the images (e.g. the object or its context), and what the methods find easy or confuse. The paper concludes with lessons learnt in the three year history of the challenge, and proposes directions for future improvement and extension.},
  keywords = {Dataset,Machine Learning},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/everinghamPascalVisualObject2009.pdf}
}

@inproceedings{girshickFastRCNN2015,
  title = {Fast {{R-CNN}}},
  booktitle = {2015 {{IEEE International Conference}} on {{Computer Vision}}},
  author = {Girshick, Ross},
  date = {2015},
  volume = {2015},
  pages = {1440--1448},
  publisher = {{IEEE}},
  doi = {10.1109/ICCV.2015.169},
  abstract = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.},
  isbn = {1550-5499},
  keywords = {Machine Learning,Model},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/girshickFastRCNN2015.pdf}
}

@article{girshickRichFeatureHierarchies2013,
  title = {Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation},
  author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
  date = {2013},
  abstract = {Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30\% relative to the previous best result on VOC 2012---achieving a mAP of 53.3\%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also compare R-CNN to OverFeat, a recently proposed sliding-window detector based on a similar CNN architecture. We find that R-CNN outperforms OverFeat by a large margin on the 200-class ILSVRC2013 detection dataset. Source code for the complete system is available at http://www.cs.berkeley.edu/\textasciitilde rbg/rcnn.},
  keywords = {Machine Learning,Model},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/girshickRichFeatureHierarchies2013.pdf}
}

@inproceedings{goilUsingMachineLearning2013,
  title = {Using Machine Learning to Blend Human and Robot Controls for Assisted Wheelchair Navigation},
  booktitle = {2013 {{IEEE}} 13th {{International Conference}} on {{Rehabilitation Robotics}} ({{ICORR}})},
  author = {Goil, Aditya and Derry, Matthew and Argall, Brenna D},
  date = {2013},
  volume = {2013},
  pages = {1--6},
  publisher = {{United States: IEEE}},
  location = {{United States}},
  doi = {doi.org/10.1109/ICORR.2013.6650454},
  abstract = {This work presents an algorithm for collaborative control of an assistive semi-autonomous wheelchair. Our approach is based on a statistical machine learning technique to learn task variability from demonstration examples. The algorithm has been developed in the context of shared-control powered wheelchairs that provide assistance to individuals with impairments that affect their control in challenging driving scenarios, like doorway navigation. We validate our algorithm within a simulation environment, and find that with relatively few demonstrations, our approach allows for safe traversal of the doorway while maintaining a high level of user control.},
  eventtitle = {2013 {{IEEE}} 13th {{International Conference}} on {{Rehabilitation Robotics}} ({{ICORR}})},
  isbn = {1945-7901},
  keywords = {Wheelchairs},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/goilUsingMachineLearning2013.pdf}
}

@misc{googlecoralCoralDevBoard2020,
  title = {Coral {{Dev Board Datasheet}}},
  author = {{Google Coral}},
  date = {2020},
  url = {https://coral.ai/static/files/Coral-Dev-Board-datasheet.pdf},
  keywords = {Hardware},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/googlecoralCoralDevBoard2020.pdf}
}

@inproceedings{habhaAutonomousWheelchairIndoorOutdoor2021,
  title = {Autonomous {{Wheelchair Indoor-Outdoor Navigation System}} through {{Accessible Routes}}},
  booktitle = {The 14th {{PErvasive Technologies Related}} to {{Assistive Environments Conference}} ({{PETRA}} 2021)},
  author = {Habha, Loiy and Trivedi, Urvish and Alqasemi, Redwan and Dubey, Rajiv},
  date = {2021},
  pages = {199--202},
  publisher = {{Association for Computing Machinery}},
  doi = {10.1145/3453892.3453997},
  abstract = {Persons with disabilities who use power wheelchairs often have difficulty finding accessible routes to their destination from their outdoor location into the desired building or from their indoor location into another destination. This challenge can be alleviated through sensory information and preloaded map of the building and its surrounding outdoor areas. In this work, a power wheelchair system is integrated with a sensory suite and an autonomous control module to navigate the wheelchair in autonomous mode. A landmark-based autonomous navigation system using Quick Response (QR) code and Global Positioning System (GPS) technology is used for automatic communication between the power wheelchair and various locations in a designated building. For the outdoor navigation system, Autonomous Wheelchair Indoor-Outdoor Navigation System (AWI-ONS) relies on GPS signals and Google Maps. For indoor navigation, QR codes are placed in several key locations inside and outside the building, such as parking lots and garages, doorways, offices, bathrooms, stores, elevators, accessible entrances, and passageways. When scanned by the onboard camera, the AWI-ONS downloads the building floorplan from the Firebase server for indoor navigation. The floor plan with QR code information generates a topological map that is made available to the user through a touch screen user interface (UI). The user can select the destination, and AWI-ONS will generate the most viable and accessible path to the desired location using modified Breadth-First Search (BFS) algorithm. The AWI-ONS is fitted with Ultrasound-based obstacle avoidance system that is designed to avoid a possible collision while navigating towards the destination.},
  isbn = {978-1-4503-8792-7},
  keywords = {Wheelchairs},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/habhaAutonomousWheelchairIndoorOutdoor2021.pdf}
}

@inproceedings{heDeepResidualLearning2016,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  date = {2016},
  volume = {2016},
  pages = {770--778},
  publisher = {{IEEE}},
  doi = {10.1109/CVPR.2016.90},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  isbn = {1063-6919},
  keywords = {Machine Learning,Model},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/heDeepResidualLearning2015.pdf}
}

@misc{intelIntelRealSenseProduct2022,
  title = {Intel {{RealSense Product Family D400 Series Datasheet}}},
  author = {{Intel}},
  date = {2022},
  url = {https://www.intelrealsense.com/wp-content/uploads/2022/03/Intel-RealSense-D400-Series-Datasheet-March-2022.pdf},
  keywords = {Hardware},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/intelIntelRealSenseProduct2022.pdf}
}

@article{jacobQuantizationTrainingNeural2017,
  title = {Quantization and {{Training}} of {{Neural Networks}} for {{Efficient Integer-Arithmetic-Only Inference}}},
  author = {Jacob, Benoit and Kligys, Skirmantas and Chen, Bo and Zhu, Menglong and Tang, Matthew and Howard, Andrew and Adam, Hartwig and Kalenichenko, Dmitry},
  date = {2017},
  doi = {10.48550/ARXIV.1712.05877},
  url = {https://arxiv.org/abs/1712.05877},
  abstract = {The rising popularity of intelligent mobile devices and the daunting computational cost of deep learning-based models call for efficient and accurate on-device inference schemes. We propose a quantization scheme that allows inference to be carried out using integer-only arithmetic, which can be implemented more efficiently than floating point inference on commonly available integer-only hardware. We also co-design a training procedure to preserve end-to-end model accuracy post quantization. As a result, the proposed quantization scheme improves the tradeoff between accuracy and on-device latency. The improvements are significant even on MobileNets, a model family known for run-time efficiency, and are demonstrated in ImageNet classification and COCO detection on popular CPUs.},
  keywords = {Machine Learning},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/jacobQuantizationTrainingNeural2017.pdf}
}

@inproceedings{jainAutomatedPerceptionSafe2014,
  title = {Automated Perception of Safe Docking Locations with Alignment Information for Assistive Wheelchairs},
  booktitle = {{{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}}},
  author = {Jain, Siddarth and Argall, Brenna},
  date = {2014},
  pages = {4997--5002},
  publisher = {{IEEE}},
  doi = {doi.org/10.1109/IROS.2014.6943272},
  abstract = {There are basic manuvering tasks with a powered wheelchair, like docking under a table and passage through a doorway or narrow hallway, which can be difficult for users with severe motor impairments - not only because of limitations in their own motor control, but also because of limitations in the control interfaces available to them. Robot automation can help transfer some of this control burden from the user to the machine. This work presents an algorithm for the automated detection of safe docking locations at rectangular and circular docking structures (tables, desks) with proper alignment information using 3D point cloud data. The safe docking locations can then be provided as goals to an autonomous path planner, within the context of providing adaptive driving assistance for powered wheelchair users. We evaluate the performance of our algorithm with systematic testing on several docking structures, observed from varied viewpoints.},
  isbn = {2153-0858},
  keywords = {From Survey,Wheelchairs},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/jainAutomatedPerceptionSafe2014.pdf}
}

@inproceedings{jiadengImageNetLargescaleHierarchical2009,
  title = {{{ImageNet}}: {{A}} Large-Scale Hierarchical Image Database},
  booktitle = {2009 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {{Jia Deng} and {Wei Dong} and Socher, Richard and {Li-Jia Li} and {Kai Li} and {Li Fei-Fei}},
  date = {2009},
  pages = {248--255},
  publisher = {{IEEE}},
  doi = {10.1109/CVPR.2009.5206848},
  abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called "ImageNet", a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
  isbn = {1063-6919},
  keywords = {Dataset,Machine Learning},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/jiadengImageNetLargescaleHierarchical2009.pdf}
}

@video{karpathyTeslaAIDay2021,
  title = {Tesla {{AI Day}}},
  editor = {Karpathy, Andrej},
  date = {2021},
  url = {youtube.com/watch?v=j0z4FweCy4M},
  editortype = {director},
  keywords = {Cars}
}

@article{kaufmannBraincomputerInterfaceBased2014,
  title = {Toward Brain-Computer Interface Based Wheelchair Control Utilizing Tactually-Evoked Event-Related Potentials},
  author = {Kaufmann, Tobias and Herweg, Andreas and Kübler, Andrea},
  date = {2014},
  journaltitle = {Journal of neuroengineering and rehabilitation},
  shortjournal = {J NEUROENG REHABIL},
  volume = {11},
  number = {1},
  pages = {7--7},
  publisher = {{LONDON: BMC}},
  location = {{LONDON}},
  issn = {1743-0003},
  doi = {10.1186/1743-0003-11-7},
  abstract = {Background: People with severe disabilities, e. g. due to neurodegenerative disease, depend on technology that allows for accurate wheelchair control. For those who cannot operate a wheelchair with a joystick, brain-computer interfaces (BCI) may offer a valuable option. Technology depending on visual or auditory input may not be feasible as these modalities are dedicated to processing of environmental stimuli (e. g. recognition of obstacles, ambient noise). Herein we thus validated the feasibility of a BCI based on tactually-evoked event-related potentials (ERP) for wheelchair control. Furthermore, we investigated use of a dynamic stopping method to improve speed of the tactile BCI system. Methods: Positions of four tactile stimulators represented navigation directions (left thigh: move left},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/kaufmannBraincomputerInterfaceBased2014.pdf}
}

@inproceedings{kondoNavigationGuidanceControl2008,
  title = {Navigation {{Guidance Control Using Haptic Feedback}} for {{Obstacle Avoidance}} of {{Omni-directional Wheelchair}}},
  booktitle = {2008 {{Symposium}} on {{Haptic Interfaces}} for {{Virtual Environment}} and {{Teleoperator Systems}}},
  author = {Kondo, Yasumasa and Miyoshi, Takanori and Terashima, Kazuhiko and Kitagawa, Hideo},
  date = {2008},
  pages = {437--444},
  publisher = {{IEEE}},
  doi = {10.1109/HAPTICS.2008.4479990},
  abstract = {It is difficult for a novice rider to navigate an omni-directional wheelchair through a narrow space such an elevator door or a path along the wall because it is always necessary to keep in mind the width of the vehicle. Such navigation can produce stress in the operator. In those cases, a navigation guidance system that appeals to human's sensation of touch can be useful. In this paper, a novel navigation guidance system using a haptic feedback joystick is proposed for the omni-directional wheelchair. The present navigation guidance system adopts a force feedback joystick in order to induce the operator to evade obstacles.},
  isbn = {2324-7347},
  keywords = {Wheelchairs},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/kondoNavigationGuidanceControl2008.pdf}
}

@article{krizhevskyImageNetClassificationDeep2012,
  title = {{{ImageNet Classification}} with {{Deep Convolutional Neural Networks}}},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey},
  date = {2012},
  journaltitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 1},
  pages = {1097--1105},
  publisher = {{NEW YORK: ACM}},
  location = {{NEW YORK}},
  issn = {0001-0782},
  doi = {10.1145/3065386},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
  keywords = {Machine Learning,Model},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/krizhevskyImageNetClassificationDeep2017.pdf}
}

@article{leamanComprehensiveReviewSmart2017,
  title = {A {{Comprehensive Review}} of {{Smart Wheelchairs}}: {{Past}}, {{Present}}, and {{Future}}},
  author = {Leaman, Jesse and {Hung Manh La}},
  date = {2017},
  journaltitle = {IEEE Transactions on Human-Machine Systems},
  shortjournal = {THMS},
  volume = {47},
  number = {4},
  pages = {486--499},
  publisher = {{PISCATAWAY: IEEE}},
  location = {{PISCATAWAY}},
  issn = {2168-2291},
  doi = {doi.org/10.1109/THMS.2017.2706727},
  abstract = {A smart wheelchair (SW) is a power wheelchair (PW) to which computers, sensors, and assistive technology are attached. In the past decade, there has been little effort to provide a systematic review of SW research. This paper aims to provide a complete state-of-the-art overview of SW research trends. We expect that the information gathered in this study will enhance awareness of the status of contemporary PW as well as SW technology and increase the functional mobility of people who use PWs. We systematically present the international SW research effort, starting with an introduction to PWs and the communities they serve. Then, we discuss in detail the SW and associated technological innovations with an emphasis on the most researched areas, generating the most interest for future research and development. We conclude with our vision for the future of SW research and how to best serve people with all types of disabilities.},
  keywords = {Survey,Wheelchairs},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/leamanComprehensiveReviewSmart2017.pdf}
}

@article{lecunGradientbasedLearningApplied1998,
  title = {Gradient-Based Learning Applied to Document Recognition},
  author = {Lecun, Y and Bottou, L and Bengio, Y and Haffner, P},
  date = {1998},
  journaltitle = {Proceedings of the IEEE},
  shortjournal = {JPROC},
  volume = {86},
  number = {11},
  pages = {2278--2324},
  publisher = {{PISCATAWAY: IEEE}},
  location = {{PISCATAWAY}},
  issn = {0018-9219},
  doi = {10.1109/5.726791},
  abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.},
  keywords = {Machine Learning,Model},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/lecunGradientbasedLearningApplied1998.pdf}
}

@article{levineNavChairAssistiveWheelchair1999,
  title = {The {{NavChair Assistive Wheelchair Navigation System}}},
  author = {Levine, S.P and Bell, D.A and Jaros, L.A and Simpson, R.C and Koren, Y and Borenstein, J},
  date = {1999},
  journaltitle = {IEEE Transactions on Rehabilitation Engineering},
  shortjournal = {T-RE},
  volume = {7},
  number = {4},
  pages = {443--451},
  publisher = {{United States: IEEE}},
  location = {{United States}},
  issn = {1063-6528},
  doi = {10.1109/86.808948},
  abstract = {The NavChair Assistive Wheelchair Navigation System is being developed to reduce the cognitive and physical requirements of operating a power wheelchair for people with wide ranging impairments that limit their access to powered mobility. The NavChair is based on a commercial wheelchair system with the addition of a DOS-based computer system, ultrasonic sensors, and an interface module interposed between the joystick and power module of the wheelchair. The obstacle avoidance routines used by the NavChair in conjunction with the ultrasonic sensors are modifications of methods originally used in mobile robotics research. The NavChair currently employs three operating modes: general obstacle avoidance, door passage, and automatic wall following. Results from performance testing of these three operating modes demonstrate their functionality. In additional to advancing the technology of smart wheelchairs, the NavChair has application to the development and testing of "shared control" systems where a human and machine share control of a system and the machine can automatically adapt to human behaviors.},
  keywords = {Wheelchairs},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/levineNavChairAssistiveWheelchair1999.pdf}
}

@article{linMicrosoftCOCOCommon2014,
  title = {Microsoft {{COCO}}: {{Common Objects}} in {{Context}}},
  author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Bourdev, Lubomir and Girshick, Ross and Hays, James and Perona, Pietro and Ramanan, Deva and Zitnick, C. Lawrence and Dollár, Piotr},
  date = {2014},
  doi = {doi.org/10.48550/arXiv.1405.0312},
  abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
  keywords = {Dataset,Machine Learning},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/linMicrosoftCOCOCommon2014.pdf}
}

@article{lugaresiMediaPipeFrameworkBuilding2019,
  title = {{{MediaPipe}}: {{A Framework}} for {{Building Perception Pipelines}}},
  author = {Lugaresi, Camillo and Tang, Jiuqiang and Nash, Hadon and McClanahan, Chris and Uboweja, Esha and Hays, Michael and Zhang, Fan and Chang, Chuo-Ling and Yong, Ming Guang and Lee, Juhyun and Chang, Wan-Teh and Hua, Wei and Georg, Manfred and Grundmann, Matthias},
  date = {2019},
  doi = {doi.org/10.48550/arXiv.1906.08172},
  abstract = {Building applications that perceive the world around them is challenging. A developer needs to (a) select and develop corresponding machine learning algorithms and models, (b) build a series of prototypes and demos, (c) balance resource consumption against the quality of the solutions, and finally (d) identify and mitigate problematic cases. The MediaPipe framework addresses all of these challenges. A developer can use MediaPipe to build prototypes by combining existing perception components, to advance them to polished cross-platform applications and measure system performance and resource consumption on target platforms. We show that these features enable a developer to focus on the algorithm or model development and use MediaPipe as an environment for iteratively improving their application with results reproducible across different devices and platforms. MediaPipe will be open-sourced at https://github.com/google/mediapipe.},
  keywords = {Framework,Machine Learning},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/lugaresiMediaPipeFrameworkBuilding2019.pdf}
}

@online{microsoftAzureKinectDK2021,
  title = {Azure {{Kinect DK Hardware Specifications}}},
  author = {{Microsoft}},
  date = {2021},
  url = {https://docs.microsoft.com/en-us/azure/Kinect-dk/hardware-specification},
  keywords = {Hardware},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/microsoftAzureKinectDK2021.pdf}
}

@article{mur-artalORBSLAM2OpenSourceSLAM2017,
  title = {{{ORB-SLAM2}}: {{An Open-Source SLAM System}} for {{Monocular}}, {{Stereo}}, and {{RGB-D Cameras}}},
  author = {Mur-Artal, Raul and Tardos, Juan D},
  date = {2017},
  journaltitle = {IEEE Transactions on Robotics},
  shortjournal = {TRO},
  volume = {33},
  number = {5},
  pages = {1255--1262},
  publisher = {{PISCATAWAY: IEEE}},
  location = {{PISCATAWAY}},
  issn = {1552-3098},
  doi = {10.1109/TRO.2017.2705103},
  abstract = {We present ORB-SLAM2, a complete simultaneous localization and mapping (SLAM) system for monocular, stereo and RGB-D cameras, including map reuse, loop closing, and relocalization capabilities. The system works in real time on standard central processing units in a wide variety of environments from small hand-held indoors sequences, to drones flying in industrial environments and cars driving around a city. Our back-end, based on bundle adjustment with monocular and stereo observations, allows for accurate trajectory estimation with metric scale. Our system includes a lightweight localization mode that leverages visual odometry tracks for unmapped regions and matches with map points that allow for zero-drift localization. The evaluation on 29 popular public sequences shows that our method achieves state-of-the-art accuracy, being in most cases the most accurate SLAM solution. We publish the source code, not only for the benefit of the SLAM community, but with the aim of being an out-of-the-box SLAM solution for researchers in other fields.},
  keywords = {Model,SLAM},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/mur-artalORBSLAM2OpenSourceSLAM2017.pdf}
}

@misc{nvidiaAmpereGA102GPU2020,
  title = {Ampere {{GA102 GPU Architecture}}},
  author = {{Nvidia}},
  date = {2020},
  url = {https://www.nvidia.com/content/PDF/nvidia-ampere-ga-102-gpu-architecture-whitepaper-v2.pdf},
  keywords = {Hardware},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/nvidiaAmpereGA102GPU2020.pdf}
}

@misc{nvidiaJetsonNanoSystemonModule2019,
  title = {Jetson {{Nano System-on-Module Data Sheet}}},
  author = {{Nvidia}},
  date = {2019},
  url = {https://developer.nvidia.com/embedded/downloads},
  keywords = {Hardware},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/nvidiaJetsonNanoSystemonModule2019.pdf}
}

@misc{nvidiaJetsonXavierNX2019,
  title = {Jetson {{Xavier NX Series System-on-Module Data Sheet}}},
  author = {{Nvidia}},
  date = {2019},
  url = {https://developer.nvidia.com/embedded/downloads},
  keywords = {Hardware},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/nvidiaJetsonXavierNX2019.pdf}
}

@misc{nvidiaTuringGPUArchitecture2018,
  title = {Turing {{GPU Architecture}}},
  author = {{Nvidia}},
  date = {2018},
  url = {https://images.nvidia.com/aem-dam/en-zz/Solutions/design-visualization/technologies/turing-architecture/NVIDIA-Turing-Architecture-Whitepaper.pdf},
  keywords = {Hardware},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/nvidiaTuringGPUArchitecture2018.pdf}
}

@article{paszkePyTorchImperativeStyle2019,
  title = {{{PyTorch}}: {{An Imperative Style}}, {{High-Performance Deep Learning Library}}},
  author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Köpf, Andreas and Yang, Edward and DeVito, Zach and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  date = {2019},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {32},
  pages = {8024--8035},
  abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.},
  keywords = {Framework,Machine Learning},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/paszkePyTorchImperativeStyle2019.pdf}
}

@article{redmonYOLO9000BetterFaster2016,
  title = {{{YOLO9000}}: {{Better}}, {{Faster}}, {{Stronger}}},
  author = {Redmon, Joseph and Farhadi, Ali},
  date = {2016},
  doi = {doi.org/10.48550/arXiv.1612.08242},
  abstract = {We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster RCNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on object detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that don't have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. But YOLO can detect more than just 200 classes},
  keywords = {Machine Learning,Model},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/redmonYOLO9000BetterFaster2016.pdf}
}

@article{redmonYOLOv3IncrementalImprovement2018,
  title = {{{YOLOv3}}: {{An Incremental Improvement}}},
  author = {Redmon, Joseph and Farhadi, Ali},
  date = {2018},
  doi = {doi.org/10.48550/arXiv.1804.02767},
  abstract = {We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network that's pretty swell. It's a little bigger than last time but more accurate. It's still fast though, don't worry. At 320x320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. When we look at the old .5 IOU mAP detection metric YOLOv3 is quite good. It achieves 57.9 mAP@50 in 51 ms on a Titan X, compared to 57.5 mAP@50 in 198 ms by RetinaNet, similar performance but 3.8x faster. As always, all the code is online at https://pjreddie.com/yolo/},
  keywords = {Machine Learning,Model},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/redmonYOLOv3IncrementalImprovement2018.pdf}
}

@article{redmonYouOnlyLook2015,
  title = {You {{Only Look Once}}: {{Unified}}, {{Real-Time Object Detection}}},
  author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  date = {2015},
  doi = {doi.org/10.48550/arXiv.1506.02640},
  abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
  keywords = {Machine Learning,Model},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/redmonYouOnlyLook2015.pdf}
}

@article{renFasterRCNNRealTime2015,
  title = {Faster {{R-CNN}}: {{Towards Real-Time Object Detection}} with {{Region Proposal Networks}}},
  author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  date = {2015},
  abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
  keywords = {Machine Learning,Model},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/renFasterRCNNRealTime2015.pdf}
}

@article{ryuDevelopmentAutonomousDriving2022,
  title = {Development of an {{Autonomous Driving Smart Wheelchair}} for the {{Physically Weak}}},
  author = {Ryu, Hye-Yeon and Kwon, Je-Seong and Lim, Jeong-Hak and Kim, A-Hyeon and Baek, Su-Jin and Kim, Jong-Wook},
  date = {2022},
  journaltitle = {Applied sciences},
  volume = {12},
  number = {1},
  pages = {377},
  publisher = {{MDPI AG}},
  issn = {2076-3417},
  doi = {doi.org/10.3390/app12010377},
  abstract = {People who have difficulty moving owing to problems in walking spend their lives assisted by wheelchairs. In the past, research has been conducted regarding the application of various technologies to electric wheelchairs for user convenience. In this study, we evaluated a method of applying an autonomous driving function and developed an autonomous driving function using ROS. An electric wheelchair with a control unit designed to enable autonomous driving was used to test the basic performance of autonomous driving. The effectiveness of the technology was confirmed by comparing the results of autonomous driving with those of manual driving on the same route. It is expected that the evaluation and improvement of the usability and ride quality as well as additional studies will help improve the mobility convenience of physically disabled persons.},
  keywords = {Wheelchairs},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/ryuDevelopmentAutonomousDriving2022.pdf}
}

@article{scudellariSelfdrivingWheelchairsDebut2017,
  title = {Self-Driving Wheelchairs Debut in Hospitals and Airports [{{News}}]},
  author = {Scudellari, Megan},
  date = {2017},
  journaltitle = {IEEE Spectrum},
  shortjournal = {SPEC},
  volume = {54},
  number = {10},
  pages = {14--14},
  publisher = {{PISCATAWAY: IEEE}},
  location = {{PISCATAWAY}},
  issn = {0018-9235},
  doi = {doi.org/10.1109/MSPEC.2017.8048827},
  abstract = {Autonomous vehicles can add a new member to their ranks-the self-driving wheelchair. This summer, two robotic wheelchairs made headlines: one at a Singaporean hospital and another at a Japanese airport. The Singapore-MIT Alliance for Research and Technology, or SMART, developed the former, first deployed in Singapore's Changi General Hospital in September 2016, where it successfully navigated the hospital's hallways. It is the latest in a string of autonomous vehicles made by SMART, including a golf cart, an electric taxi, and most recently, a scooter that zipped more than 100 MIT visitors around on tours in 2016.},
  keywords = {News,Wheelchairs},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/scudellariSelfdrivingWheelchairsDebut2017.pdf}
}

@article{simonyanVeryDeepConvolutional2014,
  title = {Very {{Deep Convolutional Networks}} for {{Large-Scale Image Recognition}}},
  author = {Simonyan, Karen and Zisserman, Andrew},
  date = {2014},
  abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
  keywords = {Machine Learning,Model},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/simonyanVeryDeepConvolutional2014.pdf}
}

@article{simpsonHowManyPeople2008,
  title = {How Many People Would Benefit from a Smart Wheelchair?},
  author = {Simpson, Richard C and LoPresti, Edmund F and Cooper, Rory A},
  date = {2008},
  journaltitle = {Journal of rehabilitation research and development},
  shortjournal = {J REHABIL RES DEV},
  volume = {45},
  number = {1},
  pages = {53--72},
  publisher = {{BALTIMORE: JOURNAL REHAB RES \& DEV}},
  location = {{BALTIMORE}},
  issn = {0748-7711},
  doi = {10.1682/JRRD.2007.01.0015},
  abstract = {Independent mobility is important, but some wheelchair users find operating existing manual or powered wheelchairs difficult or impossible. Challenges to safe, independent wheelchair use can result from various overlapping physical, perceptual, or cognitive symptoms of diagnoses such as spinal cord injury, cerebrovascular accident, multiple sclerosis, amyotrophic lateral sclerosis, and cerebral palsy. Persons with different symptom combinations can benefit from different types of assistance from a smart wheelchair and different wheelchair form factors. The sizes of these user populations have been estimated based on published estimates of the number of individuals with each of several diseases who (1) also need a wheeled mobility device and (2) have specific symptoms that could interfere with mobility device use.},
  keywords = {Wheelchairs},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/simpsonHowManyPeople2008.pdf}
}

@misc{stereolabsZEDCameraSDK2019,
  title = {{{ZED}} 2 {{Camera}} and {{SDK Overview}}},
  author = {{Stereolabs}},
  date = {2019},
  url = {https://cdn.stereolabs.com/assets/datasheets/zed2-camera-datasheet.pdf},
  keywords = {Hardware},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/stereolabsZEDCameraSDK2019.pdf}
}

@misc{stereolabsZEDMiniCamera2018,
  title = {{{ZED Mini Camera}} and {{SDK Overview}}},
  author = {{Stereolabs}},
  date = {2018},
  url = {https://cdn.stereolabs.com/assets/datasheets/zed-mini-camera-datasheet.pdf},
  keywords = {Hardware},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/stereolabsZEDMiniCamera2018.pdf}
}

@article{szegedyGoingDeeperConvolutions2014,
  title = {Going {{Deeper}} with {{Convolutions}}},
  author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  date = {2014},
  abstract = {We propose a deep convolutional neural network architecture codenamed "Inception", which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
  keywords = {Machine Learning,Model},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/szegedyGoingDeeperConvolutions2014.pdf}
}

@article{taketomiVisualSLAMAlgorithms2017,
  title = {Visual {{SLAM}} Algorithms: A Survey from 2010 to 2016},
  author = {Taketomi, Takafumi and Uchiyama, Hideaki and Ikeda, Sei},
  date = {2017},
  journaltitle = {IPSJ Transactions on Computer Vision and Applications},
  shortjournal = {IPSJ T Comput Vis Appl},
  volume = {9},
  number = {1},
  pages = {1--11},
  publisher = {{Berlin/Heidelberg: Springer Berlin Heidelberg}},
  location = {{Berlin/Heidelberg}},
  issn = {1882-6695},
  doi = {doi.org/10.1186/s41074-017-0027-2},
  abstract = {SLAM is an abbreviation for simultaneous localization and mapping, which is a technique for estimating sensor motion and reconstructing structure in an unknown environment. Especially, Simultaneous Localization and Mapping (SLAM) using cameras is referred to as visual SLAM (vSLAM) because it is based on visual information only. vSLAM can be used as a fundamental technology for various types of applications and has been discussed in the field of computer vision, augmented reality, and robotics in the literature. This paper aims to categorize and summarize recent vSLAM algorithms proposed in different research communities from both technical and historical points of views. Especially, we focus on vSLAM algorithms proposed mainly from 2010 to 2016 because major advance occurred in that period. The technical categories are summarized as follows: feature-based, direct, and RGB-D camera-based approaches.},
  keywords = {Recommended,Survey},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/VSLAM Survey.pdf}
}

@incollection{thrunRoboticMappingSurvey2002,
  title = {Robotic {{Mapping}}: {{A Survey}}},
  booktitle = {Exploring {{Artificial Intelligence}} in the {{New Millenium}}},
  author = {Thrun, Sebastian},
  date = {2002},
  publisher = {{Morgan Kaufmann}},
  url = {robots.stanford.edu/papers/thrun.mapping-tr.html},
  abstract = {This article provides a comprehensive introduction into the field of robotic mapping, with a focus on indoor mapping. It describes and compares various probabilistic techniques, as they are presently being applied to a vast array of mobile robot mapping problems. The history of robotic mapping is also described, along with an extensive list of open research problems.},
  keywords = {Recommended,Survey},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/EFK Survey.pdf}
}

@inproceedings{tomariEnhancingWheelchairControl2014,
  title = {Enhancing {{Wheelchair}}’s {{Control Operation}} of a {{Severe Impairment User}}},
  booktitle = {The 8th {{International Conference}} on {{Robotic}}, {{Vision}}, {{Signal Processing}} \& {{Power Applications}}},
  author = {Tomari, Mohd Razali Md and Kobayashi, Yoshinori and Kuno, Yoshinori},
  editor = {Mat Sakim, Harsa Amylia and Mustaffa, Mohd Tafir},
  date = {2014},
  pages = {65--72},
  publisher = {{Springer Singapore}},
  location = {{Singapore}},
  doi = {10.1007/978-981-4585-42-2},
  abstract = {Users with severe motor ability are unable to control their wheelchair using standard joystick and hence an alternative control input is preferred. However, using such an input, undoubtedly the navigation burden for the user is significantly increased. In this paper a method on how to reduce such a burden with the help of smart navigation platform is proposed. Initially, user information is inferred using an IMU sensor and a bite-like switch. Then information from the environment is obtained using combination of laser and Kinect sensors. Eventually, both information from the environment and the user is analyzed to decide the final control operation that according to the user intention, safe and comfortable to the people in the surrounding. Experimental results demonstrate the feasibility of the proposed approach.},
  isbn = {978-981-4585-42-2},
  keywords = {From Survey,Wheelchairs},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/tomariEnhancingWheelchairControl2014.pdf}
}

@software{ultralyticsYOLOv5,
  title = {{{YOLOv5}}},
  author = {{Ultralytics}},
  url = {https://github.com/ultralytics/yolov5},
  keywords = {Machine Learning,Model}
}

@inproceedings{vanderpoortenPoweredWheelchairNavigation2012,
  title = {Powered Wheelchair Navigation Assistance through Kinematically Correct Environmental Haptic Feedback},
  booktitle = {{{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  author = {Vander Poorten, E. B and Demeester, E and Reekmans, E and Philips, J and Huntemann, A and De Schutter, J},
  date = {2012},
  pages = {3706--3712},
  publisher = {{IEEE}},
  doi = {doi.org/10.1109/ICRA.2012.6225349},
  abstract = {This article introduces a set of novel haptic guidance algorithms intended to provide intuitive and reliable assistance for electric wheelchair navigation through narrow or crowded spaces. The proposed schemes take hereto the non-holonomic nature and a detailed geometry of the wheelchair into consideration. The methods encode the environment as a set of collision-free circular paths and, making use of a model-free impedance controller, `haptically' guide the user along collision-free paths or away from obstructed paths or paths that simply do not coincide with the motion intended by the user. The haptic feedback plays a central role as it establishes a fast bilateral communication channel between user and wheelchair controller and allows a direct negotiation about wheelchair motion. If found unsatisfactory, suggested trajectories can always be overruled by the user. Relying on inputs from user modeling and intention recognition schemes, the system can reduce forces needed to move along intended directions, thereby avoiding unnecessary fatigue of the user. A commercial powered wheelchair was upgraded and feasability tests were conducted to validate the proposed methods. The potential of the proposed approaches was hereby demonstrated.},
  isbn = {1050-4729},
  keywords = {From Survey,Wheelchairs},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/vanderpoortenPoweredWheelchairNavigation2012.pdf}
}

@article{vaswaniAttentionAllYou2017,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
  date = {2017},
  doi = {doi.org/10.48550/arXiv.1706.03762},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  keywords = {Machine Learning,Model},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/vaswaniAttentionAllYou2017.pdf}
}

@article{wangS2P2SelfSupervisedGoalDirected2021,
  title = {{{S2P2}}: {{Self-Supervised Goal-Directed Path Planning Using RGB-D Data}} for {{Robotic Wheelchairs}}},
  author = {Wang, Hengli and Sun, Yuxiang and Fan, Rui and Liu, Ming},
  date = {2021},
  journaltitle = {IEEE International Conference on Robotics and Automation (ICRA)},
  pages = {11422--11428},
  doi = {10.1109/ICRA48506.2021.9561314},
  url = {https://sites.google.com/view/s2p2},
  abstract = {Path planning is a fundamental capability for autonomous navigation of robotic wheelchairs. With the impressive development of deep-learning technologies, imitation learning-based path planning approaches have achieved effective results in recent years. However, the disadvantages of these approaches are twofold: 1) they may need extensive time and labor to record expert demonstrations as training data},
  keywords = {Wheelchairs},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/wangS2P2SelfSupervisedGoalDirected2021.pdf}
}

@article{wangSelfSupervisedDrivableArea2019,
  title = {Self-{{Supervised Drivable Area}} and {{Road Anomaly Segmentation Using RGB-D Data For Robotic Wheelchairs}}},
  author = {Wang, Hengli and Sun, Yuxiang and Liu, Ming},
  date = {2019},
  journaltitle = {IEEE Robotics and Automation Letters},
  shortjournal = {LRA},
  volume = {4},
  number = {4},
  pages = {4386--4393},
  publisher = {{IEEE}},
  issn = {2377-3766},
  doi = {doi.org/10.1109/LRA.2019.2932874},
  abstract = {The segmentation of drivable areas and road anomalies are critical capabilities to achieve autonomous navigation for robotic wheelchairs. The recent progress of semantic segmentation using deep learning techniques has presented effective results. However, the acquisition of large-scale datasets with hand-labeled ground truth is time-consuming and labor-intensive, making the deep learning-based methods often hard to implement in practice. We contribute to the solution of this problem for the task of drivable area and road anomaly segmentation by proposing a self-supervised learning approach. We develop a pipeline that can automatically generate segmentation labels for drivable areas and road anomalies. Then, we train RGB-D data-based semantic segmentation neural networks and get predicted labels. Experimental results show that our proposed automatic labeling pipeline achieves an impressive speed-up compared to manual labeling. In addition, our proposed self-supervised approach exhibits more robust and accurate results than the state-of-the-art traditional algorithms as well as the state-of-the-art self-supervised algorithms.},
  keywords = {Wheelchairs},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/wangSelfSupervisedDrivableArea2019.pdf}
}

@inproceedings{wanUnscentedKalmanFilter2000,
  title = {The Unscented {{Kalman}} Filter for Nonlinear Estimation},
  booktitle = {Proceedings of the {{IEEE}} 2000 {{Adaptive Systems}} for {{Signal Processing}}, {{Communications}}, and {{Control Symposium}}},
  author = {Wan, E.A and Van Der Merwe, R},
  date = {2000},
  pages = {153--158},
  publisher = {{IEEE}},
  doi = {10.1109/ASSPCC.2000.882463},
  abstract = {This paper points out the flaws in using the extended Kalman filter (EKE) and introduces an improvement, the unscented Kalman filter (UKF), proposed by Julier and Uhlman (1997). A central and vital operation performed in the Kalman filter is the propagation of a Gaussian random variable (GRV) through the system dynamics. In the EKF the state distribution is approximated by a GRV, which is then propagated analytically through the first-order linearization of the nonlinear system. This can introduce large errors in the true posterior mean and covariance of the transformed GRV, which may lead to sub-optimal performance and sometimes divergence of the filter. The UKF addresses this problem by using a deterministic sampling approach. The state distribution is again approximated by a GRV, but is now represented using a minimal set of carefully chosen sample points. These sample points completely capture the true mean and covariance of the GRV, and when propagated through the true nonlinear system, captures the posterior mean and covariance accurately to the 3rd order (Taylor series expansion) for any nonlinearity. The EKF in contrast, only achieves first-order accuracy. Remarkably, the computational complexity of the UKF is the same order as that of the EKF. Julier and Uhlman demonstrated the substantial performance gains of the UKF in the context of state-estimation for nonlinear control. Machine learning problems were not considered. We extend the use of the UKF to a broader class of nonlinear estimation problems, including nonlinear system identification, training of neural networks, and dual estimation problems. In this paper, the algorithms are further developed and illustrated with a number of additional examples.},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/wanUnscentedKalmanFilter2000.pdf}
}

@article{weiMmWaveRadarVision2022,
  title = {{{MmWave Radar}} and {{Vision Fusion}} for {{Object Detection}} in {{Autonomous Driving}}: {{A Review}}},
  author = {Wei, Zhiqing and Zhang, Fengkai and Chang, Shuo and Liu, Yangyang and Wu, Huici and Feng, Zhiyong},
  date = {2022},
  journaltitle = {Sensors},
  volume = {22},
  number = {7},
  issn = {1424-8220},
  doi = {10.3390/s22072542},
  abstract = {With autonomous driving developing in a booming stage, accurate object detection in complex scenarios attract wide attention to ensure the safety of autonomous driving. Millimeter wave (mmWave) radar and vision fusion is a mainstream solution for accurate obstacle detection. This article presents a detailed survey on mmWave radar and vision fusion based obstacle detection methods. First, we introduce the tasks, evaluation criteria, and datasets of object detection for autonomous driving. The process of mmWave radar and vision fusion is then divided into three parts: sensor deployment, sensor calibration, and sensor fusion, which are reviewed comprehensively. Specifically, we classify the fusion methods into data level, decision level, and feature level fusion methods. In addition, we introduce three-dimensional(3D) object detection, the fusion of lidar and vision in autonomous driving and multimodal information fusion, which are promising for the future. Finally, we summarize this article.},
  keywords = {Cars,Survey},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/weiMmWaveRadarVision2022.pdf}
}

@inproceedings{yuLaneDepartureWarning2008,
  title = {A {{Lane Departure Warning System Based}} on {{Machine Vision}}},
  booktitle = {2008 {{IEEE Pacific-Asia Workshop}} on {{Computational Intelligence}} and {{Industrial Application}}},
  author = {Yu, Bing and Zhang, Weigong and Cai, Yingfeng},
  date = {2008},
  volume = {1},
  pages = {197--201},
  publisher = {{IEEE}},
  doi = {10.1109/PACIIA.2008.142},
  abstract = {Lane departure warning system based on machine vision is a human decision-make like solution to avoid lane departure fatalities with low cost and high reliability. In this paper, the model of vision-based lane departure warning system and the realization is described at first. Then the method of lane detection is illustrated, which is composed of three steps: image preprocessing, binary processing and dynamical threshold choosing, and linear-parabolic model fitting. After that, the solution of how to perform the departure decision-making is proposed and demonstrated. Unlike the usual TLC (Woong Kwon et al., 1999) and CCP (Risack et al., 2000) methods, the angles between lanes and the horizontal axis in captured image coordinate are used as the criterion for lane departure decision-making. At last the experiments are implemented by use of all the steps},
  keywords = {Cars},
  file = {/Users/jakobwyatt/Library/CloudStorage/OneDrive-Personal/thesis/papers/yuLaneDepartureWarning2008.pdf}
}


